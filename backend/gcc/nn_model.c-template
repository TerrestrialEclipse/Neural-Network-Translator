#include <stdint.h>
#include <string.h>
#include <math.h>
#include <stdlib.h>
#include "nn_model.h"

/*
Purpose: Applies the activation function to the input value
Arguments: An integer value to specify the activation function, a float value
Returns: The given value with the activation function applied
*/

enum
{
  af_linear = 0,
  af_sigmoid = 1,
  af_relu = 2,
  af_tanh = 3,
  af_softmax = 4
};


// TODO enum for padding (same, valid)
// TODO enum for layer type
// TODO unify variable names for Loops
// TODO unify argument names
// TODO implement missing code segments
// TODO insert new avg and max pooling
// TODO Update method signatures

static float activation_function_apply(uint8_t act_func, float value, float denominator)
{
  if (act_func == af_sigmoid)
  {
    return (1.0 / (1.0 + (float)exp(-value)));
  }
  else if (act_func == af_relu)
  {
    if (value <= 0)
      return 0;
    else
      return value;
  }
  else if (act_func == af_tanh)
  {
    float expInput = (float)exp(value);
    float negExpInput = (float)exp(-value);
    return (expInput - negExpInput) / (expInput + negExpInput);
  }
  else if (act_func == af_softmax)
  {
    float divisor = (float)exp(value);
    return divisor / denominator;
  }
  return value;
}

static float * activation_apply(float *values, uint16_t values_width, uint16_t values_height, uint16_t values_depth, uint8_t act_func)
{
  uint16_t current_x_index;
  uint16_t current_y_index;
  uint16_t current_z_index;
  uint32_t index;
  float *output = calloc(values_width * values_height * values_depth, sizeof(float));
  float denominator;

  for (current_z_index = 0; current_z_index < values_depth; current_z_index++)
  {
    denominator = 1;
    if (act_func == 4)
    {
      denominator = 0;
      for (current_y_index = 0; current_y_index < values_height; current_y_index++)
      {
        for (current_x_index = 0; current_x_index < values_width; current_x_index++)
        {
          index = current_z_index * values_width * values_height + current_y_index * values_width + current_x_index;
          denominator = denominator + (float)exp(*(values + index));
        }
      }
    }

    for (current_y_index = 0; current_y_index < values_height; current_y_index++)
    {
      for (current_x_index = 0; current_x_index < values_width; current_x_index++)
      {
        index = current_z_index * values_width * values_height + current_y_index * values_width + current_x_index;
        *(output + index) = activation_function_apply(act_func, (*(values + index)), denominator);
      }
    }
  }

  free(values);
  return output;
}

static float * bias_apply(float *input, uint16_t input_size, const float biases[], uint16_t bias_start_index)
{
  float * result = calloc(input_size, sizeof(float));
  uint16_t current_index;
  for (current_index = 0; current_index < input_size; current_index++)
  {
    *(result + current_index) = *(input + current_index) + biases[bias_start_index + current_index];
  }
  free(input);
  return result;
}

static float * dense_apply(float *input, uint16_t number_of_previous_units, uint16_t number_of_current_units, const float weights[], uint16_t weights_start_index, const float biases[], uint16_t bias_start_index, uint8_t bias_enabled, uint8_t activation_function)
{
  uint16_t previous_unit_index;
  uint16_t current_unit_index;
  float * results = calloc(number_of_current_units, sizeof(float));
  float * results_with_activation_function = calloc(number_of_current_units, sizeof(float));
  float * results_with_bias;

  for (current_unit_index = 0; current_unit_index < number_of_current_units; current_unit_index++)
  {
    /* Loops through the number of previous units to calculate the dot product. */
    for (previous_unit_index = 0; previous_unit_index < number_of_previous_units; previous_unit_index++)
    {
      *(results + current_unit_index) =
        *(results + current_unit_index) +
        *(input + previous_unit_index) *
        weights[(number_of_current_units * previous_unit_index + current_unit_index) + weights_start_index];
    }
  }

  if (bias_enabled == 1)
  {
    results_with_bias = calloc(number_of_current_units, sizeof(float));
    results_with_bias = bias_apply(results, number_of_current_units, biases, bias_start_index);
    results_with_activation_function = activation_apply(results_with_bias, 1, number_of_current_units, 1, activation_function);
  }
  else
  {
    results_with_activation_function = activation_apply(results, 1, number_of_current_units, 1, activation_function);
  }

  free(input);
  return results_with_activation_function;
}

// TODO: TEST THIS METHOD
static float *padding_zero_apply(float *input, uint16_t input_width, uint16_t input_height, uint16_t input_depth, uint16_t pool_width, uint16_t pool_height)
{
  uint16_t current_x_index;
  uint16_t current_y_index;
  uint16_t current_z_index;
  uint16_t output_index;
  uint16_t input_index;

  uint16_t vertical_padding = padding_calculate_size(pool_height);
  uint16_t horizontal_padding = padding_calculate_size(pool_width);

  //! Create own arguments for these variables and move this method call to the predict method
  uint16_t output_width = padding_calculate_output_size(input_width, horizontal_padding);
  uint16_t output_height = padding_calculate_output_size(input_height, vertical_padding);
  uint16_t output_depth = input_depth;

  float *output = calloc(output_width * output_height * output_depth, sizeof(float));

  for (current_z_index = 0; current_z_index < output_depth; current_z_index++)
  {
    input_index = current_z_index * output_depth;
    for (current_y_index = 0; current_y_index < output_height; current_y_index++)
    {
      for (current_x_index = 0; current_x_index < output_width; current_x_index++)
      {
        if (current_y_index >= vertical_padding && current_y_index < (output_height - vertical_padding) && current_x_index >= horizontal_padding && current_x_index < (output_width - horizontal_padding))
        {
          output_index = current_y_index * output_width + current_x_index + current_z_index * (output_height * output_width);
          *(output + output_index) = *(input + input_index);
          input_index = input_index + 1;
        }
      }
    }
  }
  free(input);
  return output;
}

static float *pooling_avg_apply(float *input, uint16_t input_width, uint16_t input_height, uint16_t pool_width, uint16_t pool_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint16_t output_width, uint16_t output_height, uint16_t output_depth)
{
  uint16_t current_z_index;
  uint16_t vertical_step_index;
  uint16_t horizontal_step_index;
  uint16_t vertical_pool_index;
  uint16_t horizontal_pool_index;
  uint16_t input_index;
  uint16_t output_index;

  float result;
  float *output = calloc(output_height * output_width * output_depth, sizeof(float));

  for (current_z_index = 0; current_z_index < output_width; current_z_index++)
  {
    for (vertical_step_index = 0; vertical_step_index < output_height; vertical_step_index += vertical_stride)
    {
      for (horizontal_step_index = 0; horizontal_step_index < output_width; horizontal_step_index += horizontal_stride)
      {
        result = 0;
        for (vertical_pool_index = 0; vertical_pool_index < pool_height; vertical_pool_index++)
        {
          for (horizontal_pool_index = 0; horizontal_pool_index < pool_width; horizontal_pool_index++)
          {
            input_index = horizontal_pool_index + vertical_pool_index * input_width + vertical_step_index * input_width + horizontal_step_index + current_z_index * (input_width * input_height);
            result = result + *(input + input_index);
          }
        }
        output_index = (horizontal_step_index + vertical_step_index * pool_width) + current_z_index * (output_height * output_width);
        *(output + output_index) = (float)result / (pool_width * pool_height);
      }
    }
  }
  free(input);
  return output;
}

static float *pooling_max_apply(float *input, uint16_t input_width, uint16_t input_height, uint16_t pool_width, uint16_t pool_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint16_t output_width, uint16_t output_height, uint16_t output_depth)
{
  uint16_t current_z_index;
  uint16_t vertical_step_index;
  uint16_t horizontal_step_index;
  uint16_t vertical_pool_index;
  uint16_t horizontal_pool_index;
  uint16_t input_index;
  uint16_t output_index;
  float result;
  uint16_t initial_result_index;
  float *output = calloc(output_height * output_width, sizeof(float));

  for (current_z_index = 0; current_z_index < output_depth; current_z_index++)
  {
    for (vertical_step_index = 0; vertical_step_index < output_height; vertical_step_index += vertical_stride)
    {
      for (horizontal_step_index = 0; horizontal_step_index < output_width; horizontal_step_index += horizontal_stride)
      {
        initial_result_index = (vertical_step_index * input_width + horizontal_step_index) + current_z_index * (input_width * input_height);
        result = *(input + initial_result_index);
        for (vertical_pool_index = 0; vertical_pool_index < pool_height; vertical_pool_index++)
        {
          for (horizontal_pool_index = 1; horizontal_pool_index < pool_width; horizontal_pool_index++)
          {
            input_index = horizontal_pool_index + vertical_pool_index * input_width + vertical_step_index * input_width + horizontal_step_index + current_z_index * (input_width * input_height);
            if (*(input + input_index) > result)
            {
              result = *(input + input_index);
            }
          }
        }
        output_index = (horizontal_step_index + vertical_step_index * pool_width) + current_z_index * (output_width * output_height);
        *(output + output_index) = result;
      }
    }
  }
  free(input);
  return output;
}

static float * pooling_apply(uint8_t pooling_type, float *input, uint16_t input_width, uint16_t input_height, uint16_t input_depth, uint16_t output_width, uint16_t output_height, uint16_t output_depth, uint16_t pool_width, uint16_t pool_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint8_t padding)
{
  uint16_t vertical_padding = padding_calculate_size(pool_height);
  uint16_t horizontal_padding = padding_calculate_size(pool_width);

  if (padding == 1)
  {
    input = padding_zero_apply(input, input_width, input_height, input_depth, pool_width, pool_height);
    /* Update input_width and input_height to the new padded size */
    //! These function calls cannot be moved to the backend since this is only a temporary transformation of the matrix
    input_width = padding_calculate_output_size(input_width, horizontal_padding);
    input_height = padding_calculate_output_size(input_height, vertical_padding);
  }

  if (pooling_type == 3)
  {
    return pooling_max_apply(input, input_width, input_height, pool_width, pool_height, horizontal_stride, vertical_stride, output_width, output_height, output_depth);
  }
  else if (pooling_type == 4)
  {
    return pooling_avg_apply(input, input_width, input_height, pool_width, pool_height, horizontal_stride, vertical_stride, output_width, output_height, output_depth);
  }
  else
  {
    return input;
  }
}

static uint16_t padding_calculate_size(uint16_t pool_dimension)
{
  return (pool_dimension - 1) / 2;
}

static uint16_t padding_calculate_output_size(uint16_t input_dimension, uint16_t padding_dimension)
{
  return input_dimension + padding_dimension * 2;
}

static float *padding_values_apply(float *input, uint16_t input_width, uint16_t input_height, uint16_t input_depth, uint16_t number_of_padding_layers)
{
  uint16_t current_z_index;
  uint16_t current_padding_layer_index;
  uint16_t output_width = input_width;
  uint16_t output_height = input_height;
  uint16_t output_depth = input_depth;
  int16_t input_index;
  int16_t output_index;
  float *output;
  uint16_t output_x_index;
  uint16_t output_y_index;
  uint16_t output_z_index;

  for (current_padding_layer_index = 0; current_padding_layer_index < number_of_padding_layers; current_padding_layer_index++)
  {
    input_index = 0;
    output_width = output_width + 2;
    output_height = output_height + 2;
    output = calloc(output_width * output_height * output_depth, sizeof(float));

    //Copy values from input to output
    for (current_z_index = 0; current_z_index < output_depth; current_z_index++)
    {
      // Do you have to update the input_index here? Currently I don't think so...
      for (output_x_index = 0; output_x_index < output_height; output_x_index++)
      {
        for (output_y_index = 0; output_y_index < output_width; output_y_index++)
        {
          if (!(output_y_index < 1 || output_y_index > input_width || output_x_index < 1 || output_x_index > input_height))
          {
            output_index = output_x_index * output_width + output_y_index + current_z_index * (output_width * output_height);
            output[output_index] = input[input_index];
            input_index = input_index + 1;
          }
        }
      }
    }
    for (current_z_index = 0; current_z_index < input_depth; current_z_index++)
    {
      // Copy upper row and extend upper corners
      for (output_x_index = 0; output_x_index < output_width; output_x_index++)
      {
        input_index = output_x_index - 1 + current_z_index * (input_width * input_height);

        if (input_index < 0)
        {
          input_index = 0;
        }

        if (input_index >= input_width)
        {
          input_index = input_width - 1;
        }
        input_index = input_index + current_z_index * (input_width * input_height);
        output_index = output_x_index + current_z_index * (output_width * output_height);
        output[output_index] = input[input_index];
      }
    }

    // Copy lower row and extend lower corners
    for (current_z_index = 0; current_z_index < input_depth; current_z_index++)
    {
      for (output_x_index = 0; output_x_index < output_width; output_x_index++)
      {
        output_index = output_height * output_width - output_width + output_x_index;
        input_index = input_height * input_width - input_width + output_x_index - 1;

        if (input_index < input_height * input_width - input_width || input_index < 0)
        {
          input_index = input_height * input_width - input_width;
        }

        if (input_index >= input_height * input_width)
        {
          input_index = input_height * input_width - 1;
        }
        input_index = input_index + current_z_index * (input_width * input_height);
        output_index = output_x_index + current_z_index * (output_width * output_height);
        output[output_index] = input[input_index];
      }
    }
    //Extend left side (without corners)
    for (current_z_index = 0; current_z_index < input_depth; current_z_index++)
    {
      for (output_y_index = 1; output_y_index < output_height - 1; output_y_index++)
      {
        output_index = output_y_index * output_width;
        input_index = (output_y_index - 1) * input_width;

        if (input_index < 0)
        {
          input_index = 0;
        }

        input_index = input_index + current_z_index * (input_width * input_height);
        output_index = output_x_index + current_z_index * (output_width * output_height);
        output[output_index] = input[input_index];
      }
    }
    // Extend right side (without corners)
    for (current_z_index = 0; current_z_index < input_depth; current_z_index++)
    {
      for (output_y_index = 1; output_y_index < output_height - 1; output_y_index++)
      {
        output_index = output_y_index * output_width + output_width - 1;
        input_index = (output_y_index - 1) * input_width + input_width - 1;

        input_index = input_index + current_z_index * (input_width * input_height);
        output_index = output_x_index + current_z_index * (output_width * output_height);
        output[output_index] = input[input_index];
      }
    }

    input_width = output_width;
    input_height = output_height;
    free(input);
    input = output;
  }

  return input;
}

/*
static float * convolution_apply(float * input, uint16_t input_width, uint16_t input_height, uint16_t input_depth, const float kernel[], uint16_t kernel_start_index, uint16_t kernel_width, uint16_t kernel_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint16_t horizontal_dilation_rate, uint16_t vertical_dilation_rate, uint16_t output_width, uint16_t output_height, uint16_t output_depth)
{
  uint16_t input_y_index;
  uint16_t input_x_index;
  uint16_t input_z_index;
  uint16_t initial_result_index;
  uint16_t kernel_row_index;
  uint16_t kernel_column_index;
  uint16_t input_index;
  uint16_t kernel_index;
  uint16_t output_index = 0;
  float result;
  float *output;

  //! TODO Calculate the number of padding_layers later
  uint16_t number_of_padding_layers = 1;

  input = padding_values_apply(input, input_width, input_height, input_depth, number_of_padding_layers);


  input_width = input_width + number_of_padding_layers * 2;
  input_height = input_height + number_of_padding_layers * 2;

  output = calloc(output_height * output_width * output_depth, sizeof(float));

  for (input_z_index = 0; input_z_index < input_depth; input_z_index++)
  {
    //Loop through input matrix
    for (input_y_index = 0; input_y_index < input_height - kernel_height; input_y_index += vertical_stride)
    {
      //Loop through input matrix
      for (input_x_index = 0; input_x_index < input_width - kernel_width; input_x_index += horizontal_stride)
      {
        initial_result_index = (input_y_index * input_width + input_x_index) + input_z_index * (input_width * input_height);
        result = 0;
        for (kernel_row_index = 0; kernel_row_index < kernel_height; kernel_row_index = kernel_row_index + vertical_dilation_rate)
        {
          for (kernel_column_index = 0; kernel_column_index < kernel_width; kernel_column_index = kernel_column_index + horizontal_dilation_rate)
          {
            input_index = kernel_column_index + kernel_row_index * input_width + input_y_index * input_width + initial_result_index;
            kernel_index =  kernel_column_index + kernel_row_index * kernel_width + kernel_start_index;
            result = result + (*(input + input_index) * kernel[kernel_index]);
          }
        }
        *(output + output_index) = result;
        output_index++;
      }
    }
  }

  free(input);

  // TODO: Implement bias function call here
  // TODO: Implement activation function call here
  return output;
}
*/

/*
Purpose: Generates the output predictions for the input samples.
Arguments: A float array
Returns: A float (will be extended in future versions)
*/
float *predict(float *input)
{
  uint16_t current_layer_index;

  /* Loops through each layer of the neural network.
  The initial value is set to 1, since the layer at index 0 is the input layer
  and there is no transformation required at the input layer level. */
  for (current_layer_index = 1; current_layer_index < NUMBER_OF_LAYERS; current_layer_index++)
  {
    //Dense
    if (LAYER_TYPE[current_layer_index - 1] == 1)
    {
      uint8_t activation_function = ACTIVATION_FUNCTION[current_layer_index - 1];
      uint16_t number_of_previous_units = LAYER_OUTPUT_HEIGHT[current_layer_index - 1];
      uint16_t number_of_current_units = LAYER_OUTPUT_HEIGHT[current_layer_index];
      uint8_t bias_enabled = BIAS_ENABLED[current_layer_index - 1];
      uint16_t bias_start_index = BIASES_START_INDEX[current_layer_index - 1];
      uint16_t weights_start_index = WEIGHTS_START_INDEX[current_layer_index - 1];

      input = dense_apply(input, number_of_previous_units, number_of_current_units, WEIGHTS, weights_start_index, BIASES, bias_start_index, bias_enabled, activation_function);
    }
    //Max and avg pooling
    else if (LAYER_TYPE[current_layer_index - 1] == 3 || LAYER_TYPE[current_layer_index - 1] == 4)
    {
      uint16_t input_width = LAYER_OUTPUT_WIDTH[current_layer_index - 1];
      uint16_t input_height = LAYER_OUTPUT_HEIGHT[current_layer_index - 1];
      uint16_t input_depth = LAYER_OUTPUT_DEPTH[current_layer_index - 1];

      uint16_t output_width = LAYER_OUTPUT_WIDTH[current_layer_index];
      uint16_t output_height = LAYER_OUTPUT_HEIGHT[current_layer_index];
      uint16_t output_depth = LAYER_OUTPUT_DEPTH[current_layer_index];

      uint16_t pool_width = POOL_WIDTH[current_layer_index];
      uint16_t pool_height = POOL_HEIGHT[current_layer_index];
      uint16_t horizontal_stride = HORIZONTAL_STRIDE[current_layer_index];
      uint16_t vertical_stride = VERTICAL_STRIDE[current_layer_index];
      uint8_t padding = PADDING[current_layer_index];

      input = pooling_apply(LAYER_TYPE[current_layer_index - 1], input, input_width, input_height, input_depth, output_width, output_height, output_depth, pool_width, pool_height, horizontal_stride, vertical_stride, padding);
    }
    //Activation
    else if (LAYER_TYPE[current_layer_index - 1] == 6)
    {
      uint16_t input_width = LAYER_OUTPUT_WIDTH[current_layer_index - 1];
      uint16_t input_height = LAYER_OUTPUT_HEIGHT[current_layer_index - 1];
      uint16_t input_depth = LAYER_OUTPUT_DEPTH[current_layer_index -1];
      uint8_t activation_function = ACTIVATION_FUNCTION[current_layer_index - 1];
      input = activation_apply(input, input_width, input_height, input_depth, activation_function);
    }
    //Convolution
    /*
    else if(LAYER_TYPE[current_layer_index -1] == 5)
    {
      uint16_t input_width = LAYER_OUTPUT_WIDTH[current_layer_index - 1];
      uint16_t input_height = LAYER_OUTPUT_HEIGHT[current_layer_index - 1];
      uint16_t input_depth = LAYER_OUTPUT_DEPTH[current_layer_index - 1];

      uint16_t kernel_width = KERNEL_WIDTH[current_layer_index-1];
      uin16_t kernel_height = KERNEL_HEIGHT[current_layer_index-1];
      uint16_t kernel_start_index = KERNEL_VALUES_START_INDEX[current_layer_index-1];

      uint16_t horizontal_stride = HORIZONTAL_STRIDE[current_layer_index];
      uint16_t vertical_stride = VERTICAL_STRIDE[current_layer_index];

      uint16_t output_width = LAYER_OUTPUT_WIDTH[current_layer_index];
      uint16_t output_height = LAYER_OUTPUT_HEIGHT[current_layer_index];
      uint16_t output_depth = LAYER_OUTPUT_DEPTH[current_layer_index];

      input = convolution_apply(input, input_width, input_height, input_depth, KERNEL_VALUES, kernel_start_index, kernel_width, kernel_height, horizontal_stride, vertical_stride, output_width, output_height, output_depth);

    }
    */
  }

  /* We do not need to do anything for flatten layers since we're already using a flattened array structure
      and the calculation of the proper sizing is already performed by the python backend. */

  return input;
}