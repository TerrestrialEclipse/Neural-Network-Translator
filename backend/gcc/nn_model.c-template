#include <stdint.h>
#include <string.h>
#include <math.h>
#include <stdlib.h>
#include "nn_model.h"

/*
Purpose: Applies the activation function to the input value
Arguments: An integer value to specify the activation function, a float value
Returns: The given value with the activation function applied
*/
static float activation_function_apply(uint8_t act_func, float value, float denominator)
{
  if (act_func == 1)
  {
    return (1.0 / (1.0 + (float)exp(-value)));
  }
  else if (act_func == 2)
  {
    if (value <= 0)
      return 0;
    else
      return value;
  }
  else if (act_func == 3)
  {
    float expInput = (float)exp(value);
    float negExpInput = (float)exp(-value);
    return (expInput - negExpInput) / (expInput + negExpInput);
  }
  else if(act_func == 4)
  {
    float divisor = (float)exp(value);
    return divisor/denominator;
  }
  return value;
}

static float * activation_apply(float * values, uint16_t values_length, uint8_t act_func, float denominator)
{
  uint16_t index;
  float * output = calloc(values_length, sizeof(float));
  for(index = 0; index < values_length; index++)
  {
    *(output + index) = activation_function_apply(act_func, *(values+index), denominator);
  }
  free(values);
  return output;
}

static float * dense_apply(float * input, uint16_t number_of_previous_units, uint16_t number_of_current_units, const float weights[], uint16_t weights_start_index, const float biases[], uint16_t bias_start_index, uint8_t bias_enabled, uint8_t activation_function)
{
  uint16_t current_unit_index;
  float * current_layer_results = calloc(number_of_current_units, sizeof(float));

  /* Initialize a variable to store the denominator for the softmax activation function.
  If the softmax activation function is used, the denominator can be calculated easily
  during the following matrix calculation and will lead to a performance increase, since
  the calculation of the softmax denominator would otherwise  require a second loop through the
  whole array later on. */
  float denominator = 0;

  for (current_unit_index = 0; current_unit_index < number_of_current_units; current_unit_index++)
  {
    uint16_t previous_unit_index;

    /* If the bias is activated for the current layer, the initial value
    of the current_layer_results array can be replaced with the bias value. */
    if (bias_enabled == 1)
    {
      *(current_layer_results+current_unit_index) = biases[bias_start_index + current_unit_index];
    }

    /* Loops through the number of previous units to calculate the dot product. */
    for (previous_unit_index = 0; previous_unit_index < number_of_previous_units; previous_unit_index++)
    {
      *(current_layer_results+current_unit_index) =
          *(current_layer_results+current_unit_index) +
          *(input + previous_unit_index) *
              weights[(number_of_current_units * previous_unit_index + current_unit_index) + weights_start_index];
    }

    /* Calculates the softmax denominator. This calculation is only required when using the softmax activation function.
    Therefore, we check if the current layer uses the softmax activation function and only perform the calculation when required.
    This improves the runtime if another activation function is used. */
    if (activation_function == 4)
    {
      denominator = denominator + (float)exp(*(current_layer_results+current_unit_index));
    }
  }

  float * results_with_act = calloc(number_of_current_units, sizeof(float));
  results_with_act = activation_apply(current_layer_results, number_of_current_units, activation_function, denominator);

  free(input);
  return results_with_act;
}

// TODO: TEST THIS METHOD
static float * padding_zero_apply(float * input, uint16_t input_width, uint16_t input_height, uint16_t pool_width, uint16_t pool_height)
{
  uint16_t current_row_index;
  uint16_t current_column_index;
  uint16_t output_index;
  uint16_t input_index = 0;
  uint16_t vertical_padding = padding_calculate_size(pool_height);
  uint16_t horizontal_padding = padding_calculate_size(pool_width);

  //! I think this calculation cannot be moved to the backend since it is only a temporary calculation
  uint16_t output_width = padding_calculate_output_size(input_width, horizontal_padding);
  uint16_t output_height = padding_calculate_output_size(input_height, vertical_padding);
  float * output = calloc(output_width * output_height, sizeof(float));

  for (current_row_index = 0; current_row_index < output_height; current_row_index++)
  {
    for (current_column_index = 0; current_column_index < output_width; current_column_index++)
    {
      if (current_row_index >= vertical_padding && current_row_index < (output_height - vertical_padding) && current_column_index >= horizontal_padding && current_column_index < (output_width - horizontal_padding))
      {
        output_index = current_row_index * output_width + current_column_index;
        *(output+output_index) = *(input + input_index);
        input_index = input_index + 1;
      }
    }
  }
  free(input);
  return output;
}

static float * pooling_avg_apply(float * input, uint16_t input_width, uint16_t pool_width, uint16_t pool_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint16_t output_width, uint16_t output_height)
{
  uint16_t vertical_step_index;
  uint16_t horizontal_step_index;
  uint16_t vertical_pool_index;
  uint16_t horizontal_pool_index;
  uint16_t input_index;
  float result;
  float * output = calloc(output_height * output_width, sizeof(float));

  for (vertical_step_index = 0; vertical_step_index < output_height; vertical_step_index += vertical_stride)
  {
    for (horizontal_step_index = 0; horizontal_step_index < output_width; horizontal_step_index += horizontal_stride)
    {
      result = 0;
      for (vertical_pool_index = 0; vertical_pool_index < pool_height; vertical_pool_index++)
      {
        for (horizontal_pool_index = 0; horizontal_pool_index < pool_width; horizontal_pool_index++)
        {
          input_index = horizontal_pool_index + vertical_pool_index * input_width + vertical_step_index * input_width + horizontal_step_index;
          result = result + *(input + input_index);
        }
      }
      *(output+(horizontal_step_index + vertical_step_index * pool_width)) = (float)result / (pool_width * pool_height);
    }
  }
  free(input);
  return output;
}

static float * pooling_max_apply(float * input, uint16_t input_width, uint16_t pool_width, uint16_t pool_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint16_t output_width, uint16_t output_height)
{
  uint16_t vertical_step_index;
  uint16_t horizontal_step_index;
  uint16_t vertical_pool_index;
  uint16_t horizontal_pool_index;
  uint16_t input_index;
  float result;
  float * output = calloc(output_height * output_width, sizeof(float));

  for (vertical_step_index = 0; vertical_step_index < output_height; vertical_step_index += vertical_stride)
  {
    for (horizontal_step_index = 0; horizontal_step_index < output_width; horizontal_step_index += horizontal_stride)
    {
      result = *(input + (vertical_step_index * input_width + horizontal_step_index));
      for (vertical_pool_index = 0; vertical_pool_index < pool_height; vertical_pool_index++)
      {
        for (horizontal_pool_index = 1; horizontal_pool_index < pool_width; horizontal_pool_index++)
        {
          input_index = horizontal_pool_index + vertical_pool_index * input_width + vertical_step_index * input_width + horizontal_step_index;
          if (*(input+input_index) > result)
          {
            result = *(input+input_index);
          }
        }
      }
      *(output+(horizontal_step_index + vertical_step_index * pool_width)) = result;
    }
  }
  free(input);
  return output;
}

static float * pooling_apply(uint8_t pooling_type,float *input, uint16_t input_width, uint16_t input_height,uint16_t output_width, uint16_t output_height, uint16_t pool_width, uint16_t pool_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint8_t padding)
{
  uint16_t vertical_padding = padding_calculate_size(pool_height);
  uint16_t horizontal_padding = padding_calculate_size(pool_width);

  //! Check with Christoph if these functions can be removed
  // uint16_t output_height = pooling_calculate_output_size(input_height, pool_height, vertical_padding, vertical_stride);
  // uint16_t output_width = pooling_calculate_output_size(input_width, pool_width, horizontal_padding, horizontal_stride);

  if (padding == 1)
  {
    input = padding_zero_apply(input, input_width, input_height, pool_width, pool_height);
    /* Update input_width and input_height to the new padded size */
    //! These function calls cannot be moved to the backend since this is only a temporary transformation of the matrix
    input_width = padding_calculate_output_size(input_width, horizontal_padding);
    input_height = padding_calculate_output_size(input_height, vertical_padding);
  }

  if(pooling_type == 0)
  {
    return pooling_max_apply(input, input_width, pool_width, pool_height, horizontal_stride, vertical_stride, output_width, output_height);
  }
  else if(pooling_type == 1)
  {
    return pooling_avg_apply(input, input_width, pool_width, pool_height, horizontal_stride, vertical_stride, output_width, output_height);
  }
  else
  {
    return input;
  }
}

//! If the above function calls can be removed, this function can be removed as well
static uint16_t pooling_calculate_output_size(uint16_t input_dimension, uint16_t pool_dimension, uint16_t padding_dimension, uint16_t stride_size)
{
  return ((input_dimension-pool_dimension + 2 * padding_dimension)/stride_size)+1;
}

static uint16_t padding_calculate_size(uint16_t pool_dimension)
{
  return (pool_dimension-1)/2;
}

static uint16_t padding_calculate_output_size(uint16_t input_dimension, uint16_t padding_dimension)
{
  return input_dimension + padding_dimension * 2;
}

static float * convolution_apply()
{
// TODO Implement convolution layer logic
}

/*
Purpose: Generates the output predictions for the input samples.
Arguments: A float array
Returns: A float (will be extended in future versions)
*/
float * predict(float * input)
{
  uint16_t current_layer_index;

  /* Loops through each layer of the neural network.
  The initial value is set to 1, since the layer at index 0 is the input layer
  and there is no transformation required at the input layer level. */
  for (current_layer_index = 1; current_layer_index < NUMBER_OF_LAYERS; current_layer_index++)
  {

    if (LAYER_TYPE[current_layer_index -1] == 1)
    {
      uint8_t activation_function = ACTIVATION_FUNCTION[current_layer_index - 1];
      uint16_t number_of_previous_units =  LAYER_OUTPUT_HEIGHT[current_layer_index-1];
      uint16_t number_of_current_units = LAYER_OUTPUT_HEIGHT[current_layer_index];
      uint8_t bias_enabled = BIAS_ENABLED[current_layer_index - 1];
      uint16_t bias_start_index = BIASES_START_INDEX[current_layer_index - 1];
      uint16_t weights_start_index = WEIGHTS_START_INDEX[current_layer_index - 1];

      input = dense_apply(input, number_of_previous_units, number_of_current_units, WEIGHTS, weights_start_index, BIASES, bias_start_index, bias_enabled, activation_function);
    }
    else if (LAYER_TYPE[current_layer_index -1] == 3 || LAYER_TYPE[current_layer_index -1] == 4)
    {
      uint16_t input_width = LAYER_OUTPUT_WIDTH[current_layer_index-1];
      uint16_t input_height = LAYER_OUTPUT_HEIGHT[current_layer_index-1];
      uint16_t output_width = LAYER_OUTPUT_WIDTH[current_layer_index];
      uint16_t output_height = LAYER_OUTPUT_HEIGHT[current_layer_index];
      uint16_t pool_width = POOL_WIDTH[current_layer_index];
      uint16_t pool_height = POOL_HEIGHT[current_layer_index];
      uint16_t horizontal_stride = HORIZONTAL_STRIDE[current_layer_index];
      uint16_t vertical_stride = VERTICAL_STRIDE[current_layer_index];
      uint8_t padding = PADDING[current_layer_index];

      input = pooling_apply(LAYER_TYPE[current_layer_index-1], input, input_width, input_height,output_width, output_height, pool_width, pool_height, horizontal_stride, vertical_stride, padding);
    }
  }

   /* We do not need to do anything for flatten layers since we're already using a flattened array structure
      and the calculation of the proper sizing is already performed by the python backend. */

  return input;
}