#include <stdint.h>
#include <string.h>
#include <math.h>
#include <stdlib.h>
#include "nn_model.h"

// Enumeration for activation function types
enum
{
  af_linear = 0,
  af_sigmoid = 1,
  af_relu = 2,
  af_tanh = 3,
  af_softmax = 4
};

// Enumeration for padding states
enum
{
  padding_valid = 0,
  padding_same = 1
};

// Enumeration for layer types
enum
{
  lt_dropout = 0,
  lt_dense = 1,
  lt_flatten = 2,
  lt_max_pooling = 3,
  lt_avg_pooling = 4,
  lt_convolution = 5,
  lt_activation = 6,
  lt_batch_normalization = 7,
  lt_bias = 8
};

/*
Purpose: Applies the activation function to the input value
Arguments:
- activation: One of the defined activation functions, such as af_linear, af_sigmoid, af_relu, ...
- value: the value to which the activation function should be applied to
- denominator: If activation_function == af_softmax the denominator must be passed - otherwise set this argument to 0
Returns: The given value with the activation function applied
*/
static float activation_function_apply(uint8_t activation, float value, float denominator)
{
  if (activation == af_sigmoid)
  {
    return (1.0 / (1.0 + (float)exp(-value)));
  }
  else if (activation == af_relu)
  {
    if (value <= 0)
      return 0;
    else
      return value;
  }
  else if (activation == af_tanh)
  {
    float expInput = (float)exp(value);
    float negExpInput = (float)exp(-value);
    return (expInput - negExpInput) / (expInput + negExpInput);
  }
  else if (activation == af_softmax)
  {
    float divisor = (float)exp(value);
    return divisor / denominator;
  }
  return value;
}

/*
Purpose: Implementation of the activation layer
Arguments:
- input: A reference to the input values for this layer
- input_columns: Number of columns of the input when seen as a matrix
- input_rows: Number of rows of the input when seen as a matrix
- input_depth: Number of the z-layers of the input when seen as a matrix
- activation: The type of the activation function
Returns: A reference to the output of the layer with the activation function applied
*/
static float * activation_apply(float * input, uint16_t input_columns, uint16_t input_rows, uint16_t input_depth, uint8_t activation)
{
  uint16_t input_column_index;
  uint16_t input_row_index;
  uint16_t input_depth_index;
  uint32_t input_index;

  float *output = calloc(input_columns * input_rows * input_depth, sizeof(float));
  float denominator;

  for (input_depth_index = 0; input_depth_index < input_depth; input_depth_index++)
  {
    denominator = 1;

    /* If the activation function is the softmax function, the denominator must be calculated
    prior the activation function can be applied.*/
    if (activation == af_softmax)
    {
      denominator = 0;
      for (input_row_index = 0; input_row_index < input_rows; input_row_index++)
      {
        for (input_column_index = 0; input_column_index < input_columns; input_column_index++)
        {
          input_index = input_depth_index * input_columns * input_rows + input_row_index * input_columns + input_column_index;
          denominator = denominator + (float)exp(*(input + input_index));
        }
      }
    }

    /* Apply the activation function for each input value */
    for (input_row_index = 0; input_row_index < input_rows; input_row_index++)
    {
      for (input_column_index = 0; input_column_index < input_columns; input_column_index++)
      {
        input_index = input_depth_index * input_columns * input_rows + input_row_index * input_columns + input_column_index;
        *(output + input_index) = activation_function_apply(activation, (*(input + input_index)), denominator);
      }
    }
  }

  free(input);
  return output;
}

/*
Purpose: Implementation of the bias layer
Arguments:
- input: A reference to the input values for this layer
- input_length: Total numbers of elements in the input array
- biases: An array containing all bias values as in BIASES (not only the bias values for this layer)
- bias_start_index: Index of the first bias value in the biases array
Returns: A reference to the output of the layer with the bias values applied
*/
static float * bias_apply(float * input, uint16_t input_length, const float biases[], uint16_t bias_start_index)
{
  float * output = calloc(input_length, sizeof(float));
  uint16_t input_index;

  /* Apply the bias to each input value */
  for (input_index = 0; input_index < input_length; input_index++)
  {
    *(output + input_index) = *(input + input_index) + biases[bias_start_index + input_index];
  }
  free(input);
  return output;
}

/*
Purpose: Implementation of the dense layer
Arguments:
- input: A reference to the input values for this layer
- number_of_previous_units: The number of units/elements of the previous layer. Must be equivalent to the lenght of the input array.
- number_of_current_units: The number of units/elements which is expected for the output.
- weights: An array containing all weight values as in WEIGHTS (not only the weight values for this layer)
- weights_start_index: Index of the first weight value in the weights array
- biases: An array containing all the bias values as in BIASES (not only the bias values for this layer)
- biases_start_index: Index of the first bias value in the biases array
- use_bias: Defines whether bias values should be applied or not
- activation: Defines the desired activation function. Use af_linear if you don't want to apply an activation function
Returns: A reference to the output of the layer with the dense function applied
*/
static float * dense_apply(float * input, uint16_t number_of_previous_units, uint16_t number_of_current_units, const float weights[], uint16_t weights_start_index, const float biases[], uint16_t bias_start_index, uint8_t use_bias, uint8_t activation)
{
  uint16_t previous_unit_index;
  uint16_t current_unit_index;
  float * output = calloc(number_of_current_units, sizeof(float));
  float * output_with_activation_function = calloc(number_of_current_units, sizeof(float));
  float * output_with_bias;

  for (current_unit_index = 0; current_unit_index < number_of_current_units; current_unit_index++)
  {
    /* Loops through the number of previous units to calculate the dot product between the weights and the units. */
    for (previous_unit_index = 0; previous_unit_index < number_of_previous_units; previous_unit_index++)
    {
      *(output + current_unit_index) =
        *(output + current_unit_index) +
        *(input + previous_unit_index) *
        weights[(number_of_current_units * previous_unit_index + current_unit_index) + weights_start_index];
    }
  }

  /* We need to differentiate if the bias is activated or not, since it also changes the input of the activation function */
  if (use_bias == 1)
  {
    output_with_bias = calloc(number_of_current_units, sizeof(float));
    output_with_bias = bias_apply(output, number_of_current_units, biases, bias_start_index);
    output_with_activation_function = activation_apply(output_with_bias, 1, number_of_current_units, 1, activation);
  }
  else
  {
    output_with_activation_function = activation_apply(output, 1, number_of_current_units, 1, activation);
  }

  free(input);
  return output_with_activation_function;
}

/*
Purpose: Applies a padding with zeros around the input matrix
Arguments:
- input: A reference to the input values
- input_columns: The number of columns when the input is seen as a matrix
- input_rows: The number of rows when the input is seen as a matrix
- input_depth: The number of z-layers when the input is seen as a three dimensional matrix
- pool_size_width: The width of the filter/pool
- pool_size_height: The height of the filter/pool
Returns: A reference to the output as matrix with zero padding values applied
*/
static float * padding_zero_apply(float * input, uint16_t input_columns, uint16_t input_rows, uint16_t input_depth, uint16_t pool_size_width, uint16_t pool_size_height)
{
  uint16_t current_column_index;
  uint16_t current_row_index;
  uint16_t current_depth_index;
  uint16_t output_index;
  uint16_t input_index;

  /* Calculate the thickness of the padding */
  uint16_t padding_size_height = padding_calculate_size(pool_size_height);
  uint16_t padding_size_width = padding_calculate_size(pool_size_width);

  /* Calculate the new output size with the padding applied */
  uint16_t output_columns = padding_calculate_output_size(input_columns, padding_size_width);
  uint16_t output_rows = padding_calculate_output_size(input_rows, padding_size_height);
  uint16_t output_depth = input_depth;

  float *output = calloc(output_columns * output_rows * output_depth, sizeof(float));

  /* As the output array is initialized with zeros in it, we can simply copy the input values
  in the output array. We only must make sure that we don't copy them into the padding borders. */
  for (current_depth_index = 0; current_depth_index < output_depth; current_depth_index++)
  {
    input_index = current_depth_index * output_rows * output_columns;
    for (current_row_index = 0; current_row_index < output_rows; current_row_index++)
    {
      for (current_column_index = 0; current_column_index < output_columns; current_column_index++)
      {
        if (current_row_index >= padding_size_height && current_row_index < (output_rows - padding_size_height) && current_column_index >= padding_size_width && current_column_index < (output_columns - padding_size_width))
        {
          output_index = current_row_index * output_columns + current_column_index + current_depth_index * (output_rows * output_columns);
          *(output + output_index) = *(input + input_index);
          input_index = input_index + 1;
        }
      }
    }
  }
  free(input);
  return output;
}

/*
Purpose: Implementation of the average pooling function
Arguments:
- input: A reference to the input values
- input_columns: The number of columns when the input is seen as a matrix
- input_rows: The number of rows when the input is seen as a matrix
- input_depth: The numer of z-layers when the input is seen as a three-dimensional matrix
- pool_size_width: The width of the pool/filter
- pool_size_height: The height of the pool/filter
- horizontal_stride: The horizontal stride/stepsize of the pool/filter
- vertical_stride: The vertical stride/stepsize of the pool/filter
- output_columns: The number of expected output columns when seen as a matrix
- output_rows: The number of expected output rows when seen as amatrix
Returns: A reference to the output values with the average pooling applied
*/
static float * pooling_avg_apply(float * input, uint16_t input_columns, uint16_t input_rows, uint16_t input_depth, uint16_t pool_size_width, uint16_t pool_size_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint16_t output_columns, uint16_t output_rows)
{
  uint16_t current_depth_index;
  uint16_t current_row_index;
  uint16_t current_column_index;
  uint16_t filter_current_row_index;
  uint16_t filter_current_column_index;
  uint16_t input_index;
  uint16_t output_index = 0;
  float result;
  uint16_t filter_position_index; // Represents the index of the upper left corner of the filter matrix
  float *output = calloc(output_rows * output_columns * output_rows, sizeof(float));

  /* Loop through the input matrix */
  for (current_depth_index = 0; current_depth_index < input_depth; current_depth_index++)
  {
    for (current_row_index = 0; current_row_index <= input_rows - pool_size_height; current_row_index += vertical_stride)
    {
      for (current_column_index = 0; current_column_index <= input_columns - pool_size_width; current_column_index += horizontal_stride)
      {
        filter_position_index = (current_row_index * input_columns + current_column_index) + current_depth_index * (input_columns * input_rows);
        result = 0;

        /* Loop through the filter to perform the filter calculations */
        for (filter_current_row_index = 0; filter_current_row_index < pool_size_height; filter_current_row_index++)
        {
          for (filter_current_column_index = 0; filter_current_column_index < pool_size_width; filter_current_column_index++)
          {
            input_index = filter_position_index + filter_current_row_index * input_columns + filter_current_column_index;
            result = result + *(input+input_index);
          }
        }
        *(output + output_index) = (float)result / (pool_size_width*pool_size_height);
        output_index++;
      }
    }
  }
  free(input);
  return output;
}

/*
Purpose: Implementation of the max pooling function
Arguments:
- input: A reference to the input values
- input_columns: The number of columns when the input is seen as a matrix
- input_rows: The number of rows when the input is seen as a matrix
- input_depth: The numer of z-layers when the input is seen as a three-dimensional matrix
- pool_size_width: The width of the pool/filter
- pool_size_height: The height of the pool/filter
- horizontal_stride: The horizontal stride/stepsize of the pool/filter
- vertical_stride: The vertical stride/stepsize of the pool/filter
- output_columns: The number of expected output columns when seen as a matrix
- output_rows: The number of expected output rows when seen as amatrix
Returns: A reference to the output values with the max pooling applied
*/
static float * pooling_max_apply(float *input, uint16_t input_columns, uint16_t input_rows, uint16_t input_depth, uint16_t pool_size_width, uint16_t pool_size_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint16_t output_columns, uint16_t output_rows)
{
  uint16_t current_depth_index;
  uint16_t current_row_index;
  uint16_t current_column_index;
  uint16_t filter_current_row_index;
  uint16_t filter_current_column_index;
  uint16_t input_index;
  uint16_t output_index = 0;
  float result;
  uint16_t filter_position_index; // Represents the index of the upper left corner of the filter matrix
  float *output = calloc(output_rows * output_columns * output_rows, sizeof(float));

  /* Loop through the input matrix */
  for (current_depth_index = 0; current_depth_index < input_depth; current_depth_index++)
  {
    for (current_row_index = 0; current_row_index <= input_rows - pool_size_height; current_row_index += vertical_stride)
    {
      for (current_column_index = 0; current_column_index <= input_columns - pool_size_width; current_column_index += horizontal_stride)
      {
        filter_position_index = (current_row_index * input_columns + current_column_index) + current_depth_index * (input_columns * input_rows);
        result = *(input + filter_position_index);

        /* Loop through the filter to perform the filter calculations */
        for (filter_current_row_index = 0; filter_current_row_index < pool_size_height; filter_current_row_index++)
        {
          for (filter_current_column_index = 0; filter_current_column_index < pool_size_width; filter_current_column_index++)
          {
            input_index = filter_position_index + filter_current_row_index * input_columns + filter_current_column_index;
            if (*(input + input_index) > result)
            {
              result = *(input + input_index);
            }
          }
        }
        *(output + output_index) = result;
        output_index++;
      }
    }
  }
  free(input);
  return output;
}

/*
Purpose: The general function for pooling layers. Call this function if you want to apply padding before the pooling
Arguments:
- input: A reference to the input values
- input_columns: The number of columns of the input when seen as a matrix
- input_rows: The number of rows of the input when seen as a matrix
- input_depth: The number of z-layers of the input when seen as a three-dimensional matrix
- pooling_type: The desired pooling type, e.g. max or avg - Use lt_max_pooling or lt_avg_pooling here
- pool_size_width: The width of the pool/filter
- pool_size_height: The height of the pool/filter
- horizontal_stride: The horizontal stride/stepsize
- vertical_stride: The vertical stride/stepsize
- padding: The desired padding type, e.g. padding_same or padding_valid - Use padding_same to apply padding
- output_columns: The expected number of columns for the output
- output_rows: The expected number of rows for the output
Returns: A reference to the output values with the pooling applied
*/
static float * pooling_apply(float *input, uint16_t input_columns, uint16_t input_rows, uint16_t input_depth, uint8_t pooling_type, uint16_t pool_size_width, uint16_t pool_size_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint8_t padding, uint16_t output_columns, uint16_t output_rows)
{
  uint16_t padding_size_height;
  uint16_t padding_size_width;

  if (padding == padding_same)
  {
    input = padding_zero_apply(input, input_columns, input_rows, input_depth, pool_size_width, pool_size_height);

    padding_size_height = padding_calculate_size(pool_size_height);
    padding_size_width = padding_calculate_size(pool_size_width);

    /* Update input_columns and input_rows to the new padded size */
    input_columns = padding_calculate_output_size(input_columns, padding_size_width);
    input_rows = padding_calculate_output_size(input_rows, padding_size_height);
  }

  if (pooling_type == lt_max_pooling)
  {
    return pooling_max_apply(input, input_columns, input_rows, input_depth, pool_size_width, pool_size_height, horizontal_stride, vertical_stride, output_columns, output_rows);
  }
  else if (pooling_type == lt_avg_pooling)
  {
    return pooling_avg_apply(input, input_columns, input_rows, input_depth, pool_size_width, pool_size_height, horizontal_stride, vertical_stride, output_columns, output_rows);
  }
  else
  {
    return input;
  }
}

/*
Purpose: Calculates the "thickness" of the padding
Arguments:
- pool_size: Either the horizontal or the vertical pool size
Returns: The "thickness" of the horizontal or vertical padding according to the given pool size
*/
static uint16_t padding_calculate_size(uint16_t pool_size)
{
  return (pool_size - 1) / 2;
}

/*
Purpose: Calculates the width or height of after the padding is applied
Arguments:
- input_size: The width or height of the matrix
- padding_size: The thickness of the padding
Returns: The new width or height of the matrix after applying the padding values
*/
static uint16_t padding_calculate_output_size(uint16_t input_size, uint16_t padding_size)
{
  return input_size + padding_size * 2;
}

/*
Purpose: Applies a padding with the outer values around the input matrix
Arguments:
- input: A reference to the input values
- input_columns: The number of columns of the input when seen as a matrix
- input_rows: The number of rows of the input when seen as a matrix
- input_depth: The number of z-layers of the input when seen as three-dimensional matrix
- number_of_padding_layers: The thickness of the padding layer
Returns: A reference to the output values with the value padding applied
*/
static float * padding_values_apply(float *input, uint16_t input_columns, uint16_t input_rows, uint16_t input_depth, uint16_t number_of_padding_layers)
{
  uint16_t current_depth_index;
  uint16_t current_padding_layer_index;

  float *output;

  uint16_t output_columns = input_columns;
  uint16_t output_rows = input_rows;
  uint16_t output_depth = input_depth;
  uint16_t output_row_index;
  uint16_t output_column_index;
  uint16_t output_depth_index;

  int16_t input_index;
  int16_t output_index;

  for (current_padding_layer_index = 0; current_padding_layer_index < number_of_padding_layers; current_padding_layer_index++)
  {
    input_index = 0;
    output_columns = output_columns + 2;
    output_rows = output_rows + 2;
    output = calloc(output_columns * output_rows * output_depth, sizeof(float));

    //Copy values from input to output
    for (current_depth_index = 0; current_depth_index < output_depth; current_depth_index++)
    {
      for (output_row_index = 0; output_row_index < output_rows; output_row_index++)
      {
        for (output_column_index = 0; output_column_index < output_columns; output_column_index++)
        {
          if (!(output_column_index < 1 || output_column_index > input_columns || output_row_index < 1 || output_row_index > input_rows))
          {
            output_index = output_row_index * output_columns + output_column_index + current_depth_index * (output_columns * output_rows);
            output[output_index] = input[input_index];
            input_index = input_index + 1;
          }
        }
      }
    }
    for (current_depth_index = 0; current_depth_index < input_depth; current_depth_index++)
    {
      // Copy upper row and extend upper corners
      for (output_row_index = 0; output_row_index < output_columns; output_row_index++)
      {
        input_index = output_row_index - 1 + current_depth_index * (input_columns * input_rows);

        if (input_index < 0)
        {
          input_index = 0;
        }

        if (input_index >= input_columns)
        {
          input_index = input_columns - 1;
        }
        input_index = input_index + current_depth_index * (input_columns * input_rows);
        output_index = output_row_index + current_depth_index * (output_columns * output_rows);
        output[output_index] = input[input_index];
      }
    }

    // Copy lower row and extend lower corners
    for (current_depth_index = 0; current_depth_index < input_depth; current_depth_index++)
    {
      for (output_row_index = 0; output_row_index < output_columns; output_row_index++)
      {
        output_index = output_rows * output_columns - output_columns + output_row_index;
        input_index = input_rows * input_columns - input_columns + output_row_index - 1;

        if (input_index < input_rows * input_columns - input_columns || input_index < 0)
        {
          input_index = input_rows * input_columns - input_columns;
        }

        if (input_index >= input_rows * input_columns)
        {
          input_index = input_rows * input_columns - 1;
        }
        input_index = input_index + current_depth_index * (input_columns * input_rows);
        output_index = output_row_index + current_depth_index * (output_columns * output_rows);
        output[output_index] = input[input_index];
      }
    }
    //Extend left side (without corners)
    for (current_depth_index = 0; current_depth_index < input_depth; current_depth_index++)
    {
      for (output_column_index = 1; output_column_index < output_rows - 1; output_column_index++)
      {
        output_index = output_column_index * output_columns;
        input_index = (output_column_index - 1) * input_columns;

        if (input_index < 0)
        {
          input_index = 0;
        }

        input_index = input_index + current_depth_index * (input_columns * input_rows);
        output_index = output_row_index + current_depth_index * (output_columns * output_rows);
        output[output_index] = input[input_index];
      }
    }
    // Extend right side (without corners)
    for (current_depth_index = 0; current_depth_index < input_depth; current_depth_index++)
    {
      for (output_column_index = 1; output_column_index < output_rows - 1; output_column_index++)
      {
        output_index = output_column_index * output_columns + output_columns - 1;
        input_index = (output_column_index - 1) * input_columns + input_columns - 1;

        input_index = input_index + current_depth_index * (input_columns * input_rows);
        output_index = output_row_index + current_depth_index * (output_columns * output_rows);
        output[output_index] = input[input_index];
      }
    }

    input_columns = output_columns;
    input_rows = output_rows;
    free(input);
    input = output;
  }

  return input;
}

/*
Purpose: Generates the output predictions for the input samples.
Arguments:
- input: A reference to the input values as a flattened array
Returns: A reference to the output as a flattened array
*/
float * predict(float * input)
{
  uint16_t current_layer_index;

  /* Loops through each layer of the neural network.
  The initial value is set to 1, since the layer at index 0 is the input layer
  and there is no transformation required at the input layer level. */
  for (current_layer_index = 1; current_layer_index < NUMBER_OF_LAYERS; current_layer_index++)
  {
    //Dense
    if (LAYER_TYPE[current_layer_index - 1] == lt_dense)
    {
      uint8_t activation = ACTIVATION_FUNCTION[current_layer_index - 1];
      uint16_t number_of_previous_units = LAYER_OUTPUT_HEIGHT[current_layer_index - 1];
      uint16_t number_of_current_units = LAYER_OUTPUT_HEIGHT[current_layer_index];
      uint8_t use_bias = BIAS_ENABLED[current_layer_index - 1];
      uint16_t bias_start_index = BIASES_START_INDEX[current_layer_index - 1];
      uint16_t weights_start_index = WEIGHTS_START_INDEX[current_layer_index - 1];

      input = dense_apply(input, number_of_previous_units, number_of_current_units, WEIGHTS, weights_start_index, BIASES, bias_start_index, use_bias, activation);
    }
    //Max and avg pooling
    else if (LAYER_TYPE[current_layer_index - 1] == lt_max_pooling || LAYER_TYPE[current_layer_index - 1] == lt_avg_pooling)
    {
      uint16_t input_columns = LAYER_OUTPUT_WIDTH[current_layer_index - 1];
      uint16_t input_rows = LAYER_OUTPUT_HEIGHT[current_layer_index - 1];
      uint16_t input_depth = LAYER_OUTPUT_DEPTH[current_layer_index - 1];

      uint16_t output_columns = LAYER_OUTPUT_WIDTH[current_layer_index];
      uint16_t output_rows = LAYER_OUTPUT_HEIGHT[current_layer_index];
      uint16_t output_depth = LAYER_OUTPUT_DEPTH[current_layer_index];

      uint16_t pool_size_width = POOL_WIDTH[current_layer_index];
      uint16_t pool_size_height = POOL_HEIGHT[current_layer_index];
      uint16_t horizontal_stride = HORIZONTAL_STRIDE[current_layer_index];
      uint16_t vertical_stride = VERTICAL_STRIDE[current_layer_index];
      uint8_t padding = PADDING[current_layer_index];
      uint8_t pooling_type = LAYER_TYPE[current_layer_index - 1];
      input = pooling_apply(input, input_columns, input_rows, input_depth, pooling_type, pool_size_width, pool_size_height, horizontal_stride, vertical_stride, padding, output_columns, output_rows);
    }
    //Activation
    else if (LAYER_TYPE[current_layer_index - 1] == lt_activation)
    {
      uint16_t input_columns = LAYER_OUTPUT_WIDTH[current_layer_index - 1];
      uint16_t input_rows = LAYER_OUTPUT_HEIGHT[current_layer_index - 1];
      uint16_t input_depth = LAYER_OUTPUT_DEPTH[current_layer_index -1];
      uint8_t activation = ACTIVATION_FUNCTION[current_layer_index - 1];
      input = activation_apply(input, input_columns, input_rows, input_depth, activation);
    }

    /* We do not need to do anything for flatten layers since we're already using a flattened array structure
      and the calculation of the proper sizing is already performed by the python backend. */
  }

  return input;
}