#include <stdint.h>
#include <string.h>
#include <math.h>
#include <stdlib.h>
#include "nn_model.h"

// Enumeration for activation function types
enum
{
  af_linear = 0,
  af_sigmoid = 1,
  af_relu = 2,
  af_tanh = 3,
  af_softmax = 4
};

// Enumeration for padding states
enum
{
  padding_valid = 0,
  padding_same = 1
};

// Enumeration for layer types
enum
{
  lt_dropout = 0,
  lt_dense = 1,
  lt_flatten = 2,
  lt_max_pooling = 3,
  lt_avg_pooling = 4,
  lt_convolution = 5,
  lt_activation = 6,
  lt_batch_normalization = 7,
  lt_bias = 8
};

/*
Purpose: Applies the activation function to the input value
Arguments:
- activation: One of the defined activation functions, such as af_linear, af_sigmoid, af_relu, ...
- value: the value to which the activation function should be applied to
- denominator: If activation_function == af_softmax the denominator must be passed - otherwise set this argument to 0
Returns: The given value with the activation function applied
*/
static float activation_function_apply(uint8_t activation, float value, float denominator)
{
  if (activation == af_sigmoid)
  {
    return (1.0 / (1.0 + (float)exp(-value)));
  }
  else if (activation == af_relu)
  {
    if (value <= 0)
      return 0;
    else
      return value;
  }
  else if (activation == af_tanh)
  {
    float expInput = (float)exp(value);
    float negExpInput = (float)exp(-value);
    return (expInput - negExpInput) / (expInput + negExpInput);
  }
  else if (activation == af_softmax)
  {
    float divisor = (float)exp(value);
    return divisor / denominator;
  }
  return value;
}

/*
Purpose: Implementation of the activation layer
Arguments:
Returns:
*/
static float * activation_apply(float * input, uint16_t input_columns, uint16_t input_rows, uint16_t input_depth, uint8_t activation)
{
  uint16_t input_column_index;
  uint16_t input_row_index;
  uint16_t input_depth_index;
  uint32_t input_index;

  float *output = calloc(input_columns * input_rows * input_depth, sizeof(float));
  float denominator;

  for (input_depth_index = 0; input_depth_index < input_depth; input_depth_index++)
  {
    denominator = 1;
    if (activation == 4)
    {
      denominator = 0;
      for (input_row_index = 0; input_row_index < input_rows; input_row_index++)
      {
        for (input_column_index = 0; input_column_index < input_columns; input_column_index++)
        {
          input_index = input_depth_index * input_columns * input_rows + input_row_index * input_columns + input_column_index;
          denominator = denominator + (float)exp(*(input + input_index));
        }
      }
    }

    for (input_row_index = 0; input_row_index < input_rows; input_row_index++)
    {
      for (input_column_index = 0; input_column_index < input_columns; input_column_index++)
      {
        input_index = input_depth_index * input_columns * input_rows + input_row_index * input_columns + input_column_index;
        *(output + input_index) = activation_function_apply(activation, (*(input + input_index)), denominator);
      }
    }
  }

  free(input);
  return output;
}

/*
Purpose: Implementation of the bias layer
Arguments:
Returns:
*/
static float * bias_apply(float * input, uint16_t input_length, const float biases[], uint16_t bias_start_index)
{
  float * output = calloc(input_length, sizeof(float));
  uint16_t input_index;
  for (input_index = 0; input_index < input_length; input_index++)
  {
    *(output + input_index) = *(input + input_index) + biases[bias_start_index + input_index];
  }
  free(input);
  return output;
}

/*
Purpose: Implementation of the dense layer
Arguments:
Returns:
*/
static float * dense_apply(float * input, uint16_t number_of_previous_units, uint16_t number_of_current_units, const float weights[], uint16_t weights_start_index, const float biases[], uint16_t bias_start_index, uint8_t use_bias, uint8_t activation)
{
  uint16_t previous_unit_index;
  uint16_t current_unit_index;
  float * output = calloc(number_of_current_units, sizeof(float));
  float * output_with_activation_function = calloc(number_of_current_units, sizeof(float));
  float * output_with_bias;

  for (current_unit_index = 0; current_unit_index < number_of_current_units; current_unit_index++)
  {
    /* Loops through the number of previous units to calculate the dot product. */
    for (previous_unit_index = 0; previous_unit_index < number_of_previous_units; previous_unit_index++)
    {
      *(output + current_unit_index) =
        *(output + current_unit_index) +
        *(input + previous_unit_index) *
        weights[(number_of_current_units * previous_unit_index + current_unit_index) + weights_start_index];
    }
  }

  if (use_bias == 1)
  {
    output_with_bias = calloc(number_of_current_units, sizeof(float));
    output_with_bias = bias_apply(output, number_of_current_units, biases, bias_start_index);
    output_with_activation_function = activation_apply(output_with_bias, 1, number_of_current_units, 1, activation);
  }
  else
  {
    output_with_activation_function = activation_apply(output, 1, number_of_current_units, 1, activation);
  }

  free(input);
  return output_with_activation_function;
}

/*
Purpose:
Arguments:
Returns:
*/
static float * padding_zero_apply(float * input, uint16_t input_columns, uint16_t input_rows, uint16_t input_depth, uint16_t pool_size_width, uint16_t pool_size_height)
{
  uint16_t current_column_index;
  uint16_t current_row_index;
  uint16_t current_depth_index;
  uint16_t output_index;
  uint16_t input_index;

  uint16_t padding_size_height = padding_calculate_size(pool_size_height);
  uint16_t padding_size_width = padding_calculate_size(pool_size_width);

  uint16_t output_columns = padding_calculate_output_size(input_columns, padding_size_width);
  uint16_t output_rows = padding_calculate_output_size(input_rows, padding_size_height);
  uint16_t output_depth = input_depth;

  float *output = calloc(output_columns * output_rows * output_depth, sizeof(float));

  for (current_depth_index = 0; current_depth_index < output_depth; current_depth_index++)
  {
    input_index = current_depth_index * output_depth;
    for (current_row_index = 0; current_row_index < output_rows; current_row_index++)
    {
      for (current_column_index = 0; current_column_index < output_columns; current_column_index++)
      {
        if (current_row_index >= padding_size_height && current_row_index < (output_rows - padding_size_height) && current_column_index >= padding_size_width && current_column_index < (output_columns - padding_size_width))
        {
          output_index = current_row_index * output_columns + current_column_index + current_depth_index * (output_rows * output_columns);
          *(output + output_index) = *(input + input_index);
          input_index = input_index + 1;
        }
      }
    }
  }
  free(input);
  return output;
}

/*
Purpose:
Arguments:
Returns:
*/
static float * pooling_avg_apply(float * input, uint16_t input_columns, uint16_t input_rows, uint16_t input_depth, uint16_t pool_size_width, uint16_t pool_size_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint16_t output_columns, uint16_t output_rows)
{
  uint16_t current_depth_index;
  uint16_t current_row_index;
  uint16_t current_column_index;
  uint16_t filter_current_row_index;
  uint16_t filter_current_column_index;
  uint16_t input_index;
  uint16_t output_index = 0;
  float result;
  uint16_t filter_position_index; // Represents the index of the upper left corner of the filter matrix
  float *output = calloc(output_rows * output_columns * output_rows, sizeof(float));

  for (current_depth_index = 0; current_depth_index < input_depth; current_depth_index++)
  {
    for (current_row_index = 0; current_row_index <= input_rows - pool_size_height; current_row_index += vertical_stride)
    {
      for (current_column_index = 0; current_column_index <= input_columns - pool_size_width; current_column_index += horizontal_stride)
      {
        filter_position_index = (current_row_index * input_columns + current_column_index) + current_depth_index * (input_columns * input_rows);
        result = 0;
        for (filter_current_row_index = 0; filter_current_row_index < pool_size_height; filter_current_row_index++)
        {
          for (filter_current_column_index = 0; filter_current_column_index < pool_size_width; filter_current_column_index++)
          {
            input_index = filter_position_index + filter_current_row_index * input_columns + filter_current_column_index;
            result = result + *(input+input_index);
          }
        }
        *(output + output_index) = (float)result / (pool_size_width*pool_size_height);
        output_index++;
      }
    }
  }
  free(input);
  return output;
}

/*
Purpose:
Arguments:
Returns:
*/
static float * pooling_max_apply(float *input, uint16_t input_columns, uint16_t input_rows, uint16_t input_depth, uint16_t pool_size_width, uint16_t pool_size_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint16_t output_columns, uint16_t output_rows)
{
  uint16_t current_depth_index;
  uint16_t current_row_index;
  uint16_t current_column_index;
  uint16_t filter_current_row_index;
  uint16_t filter_current_column_index;
  uint16_t input_index;
  uint16_t output_index = 0;
  float result;
  uint16_t filter_position_index; // Represents the index of the upper left corner of the filter matrix
  float *output = calloc(output_rows * output_columns * output_rows, sizeof(float));

  for (current_depth_index = 0; current_depth_index < input_depth; current_depth_index++)
  {
    for (current_row_index = 0; current_row_index <= input_rows - pool_size_height; current_row_index += vertical_stride)
    {
      for (current_column_index = 0; current_column_index <= input_columns - pool_size_width; current_column_index += horizontal_stride)
      {
        filter_position_index = (current_row_index * input_columns + current_column_index) + current_depth_index * (input_columns * input_rows);
        result = *(input + filter_position_index);
        for (filter_current_row_index = 0; filter_current_row_index < pool_size_height; filter_current_row_index++)
        {
          for (filter_current_column_index = 0; filter_current_column_index < pool_size_width; filter_current_column_index++)
          {
            input_index = filter_position_index + filter_current_row_index * input_columns + filter_current_column_index;
            if (*(input + input_index) > result)
            {
              result = *(input + input_index);
            }
          }
        }
        *(output + output_index) = result;
        output_index++;
      }
    }
  }
  free(input);
  return output;
}

/*
Purpose:
Arguments:
Returns:
*/
static float * pooling_apply(float *input, uint16_t input_columns, uint16_t input_rows, uint16_t input_depth, uint8_t pooling_type, uint16_t pool_size_width, uint16_t pool_size_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint8_t padding, uint16_t output_columns, uint16_t output_rows)
{
  uint16_t padding_size_height;
  uint16_t padding_size_width;

  if (padding == padding_same)
  {
    input = padding_zero_apply(input, input_columns, input_rows, input_depth, pool_size_width, pool_size_height);

    padding_size_height = padding_calculate_size(pool_size_height);
    padding_size_width = padding_calculate_size(pool_size_width);

    /* Update input_columns and input_rows to the new padded size */
    input_columns = padding_calculate_output_size(input_columns, padding_size_width);
    input_rows = padding_calculate_output_size(input_rows, padding_size_height);
  }

  if (pooling_type == lt_max_pooling)
  {
    return pooling_max_apply(input, input_columns, input_rows, input_depth, pool_size_width, pool_size_height, horizontal_stride, vertical_stride, output_columns, output_rows);
  }
  else if (pooling_type == lt_avg_pooling)
  {
    return pooling_avg_apply(input, input_columns, input_rows, input_depth, pool_size_width, pool_size_height, horizontal_stride, vertical_stride, output_columns, output_rows);
  }
  else
  {
    return input;
  }
}

/*
Purpose:
Arguments:
Returns:
*/
static uint16_t padding_calculate_size(uint16_t pool_size)
{
  return (pool_size - 1) / 2;
}

static uint16_t padding_calculate_output_size(uint16_t input_size, uint16_t padding_size)
{
  return input_size + padding_size * 2;
}

/*
Purpose:
Arguments:
Returns:
*/
static float * padding_values_apply(float *input, uint16_t input_columns, uint16_t input_rows, uint16_t input_depth, uint16_t number_of_padding_layers)
{
  uint16_t current_depth_index;
  uint16_t current_padding_layer_index;

  float *output;

  uint16_t output_columns = input_columns;
  uint16_t output_rows = input_rows;
  uint16_t output_depth = input_depth;
  uint16_t output_row_index;
  uint16_t output_column_index;
  uint16_t output_depth_index;

  int16_t input_index;
  int16_t output_index;

  for (current_padding_layer_index = 0; current_padding_layer_index < number_of_padding_layers; current_padding_layer_index++)
  {
    input_index = 0;
    output_columns = output_columns + 2;
    output_rows = output_rows + 2;
    output = calloc(output_columns * output_rows * output_depth, sizeof(float));

    //Copy values from input to output
    for (current_depth_index = 0; current_depth_index < output_depth; current_depth_index++)
    {
      for (output_row_index = 0; output_row_index < output_rows; output_row_index++)
      {
        for (output_column_index = 0; output_column_index < output_columns; output_column_index++)
        {
          if (!(output_column_index < 1 || output_column_index > input_columns || output_row_index < 1 || output_row_index > input_rows))
          {
            output_index = output_row_index * output_columns + output_column_index + current_depth_index * (output_columns * output_rows);
            output[output_index] = input[input_index];
            input_index = input_index + 1;
          }
        }
      }
    }
    for (current_depth_index = 0; current_depth_index < input_depth; current_depth_index++)
    {
      // Copy upper row and extend upper corners
      for (output_row_index = 0; output_row_index < output_columns; output_row_index++)
      {
        input_index = output_row_index - 1 + current_depth_index * (input_columns * input_rows);

        if (input_index < 0)
        {
          input_index = 0;
        }

        if (input_index >= input_columns)
        {
          input_index = input_columns - 1;
        }
        input_index = input_index + current_depth_index * (input_columns * input_rows);
        output_index = output_row_index + current_depth_index * (output_columns * output_rows);
        output[output_index] = input[input_index];
      }
    }

    // Copy lower row and extend lower corners
    for (current_depth_index = 0; current_depth_index < input_depth; current_depth_index++)
    {
      for (output_row_index = 0; output_row_index < output_columns; output_row_index++)
      {
        output_index = output_rows * output_columns - output_columns + output_row_index;
        input_index = input_rows * input_columns - input_columns + output_row_index - 1;

        if (input_index < input_rows * input_columns - input_columns || input_index < 0)
        {
          input_index = input_rows * input_columns - input_columns;
        }

        if (input_index >= input_rows * input_columns)
        {
          input_index = input_rows * input_columns - 1;
        }
        input_index = input_index + current_depth_index * (input_columns * input_rows);
        output_index = output_row_index + current_depth_index * (output_columns * output_rows);
        output[output_index] = input[input_index];
      }
    }
    //Extend left side (without corners)
    for (current_depth_index = 0; current_depth_index < input_depth; current_depth_index++)
    {
      for (output_column_index = 1; output_column_index < output_rows - 1; output_column_index++)
      {
        output_index = output_column_index * output_columns;
        input_index = (output_column_index - 1) * input_columns;

        if (input_index < 0)
        {
          input_index = 0;
        }

        input_index = input_index + current_depth_index * (input_columns * input_rows);
        output_index = output_row_index + current_depth_index * (output_columns * output_rows);
        output[output_index] = input[input_index];
      }
    }
    // Extend right side (without corners)
    for (current_depth_index = 0; current_depth_index < input_depth; current_depth_index++)
    {
      for (output_column_index = 1; output_column_index < output_rows - 1; output_column_index++)
      {
        output_index = output_column_index * output_columns + output_columns - 1;
        input_index = (output_column_index - 1) * input_columns + input_columns - 1;

        input_index = input_index + current_depth_index * (input_columns * input_rows);
        output_index = output_row_index + current_depth_index * (output_columns * output_rows);
        output[output_index] = input[input_index];
      }
    }

    input_columns = output_columns;
    input_rows = output_rows;
    free(input);
    input = output;
  }

  return input;
}

/*
Purpose: Generates the output predictions for the input samples.
Arguments: A float array
Returns: A float (will be extended in future versions)
*/
float * predict(float * input)
{
  uint16_t current_layer_index;

  /* Loops through each layer of the neural network.
  The initial value is set to 1, since the layer at index 0 is the input layer
  and there is no transformation required at the input layer level. */
  for (current_layer_index = 1; current_layer_index < NUMBER_OF_LAYERS; current_layer_index++)
  {
    //Dense
    if (LAYER_TYPE[current_layer_index - 1] == lt_dense)
    {
      uint8_t activation = ACTIVATION_FUNCTION[current_layer_index - 1];
      uint16_t number_of_previous_units = LAYER_OUTPUT_HEIGHT[current_layer_index - 1];
      uint16_t number_of_current_units = LAYER_OUTPUT_HEIGHT[current_layer_index];
      uint8_t use_bias = BIAS_ENABLED[current_layer_index - 1];
      uint16_t bias_start_index = BIASES_START_INDEX[current_layer_index - 1];
      uint16_t weights_start_index = WEIGHTS_START_INDEX[current_layer_index - 1];

      input = dense_apply(input, number_of_previous_units, number_of_current_units, WEIGHTS, weights_start_index, BIASES, bias_start_index, use_bias, activation);
    }
    //Max and avg pooling
    else if (LAYER_TYPE[current_layer_index - 1] == lt_max_pooling || LAYER_TYPE[current_layer_index - 1] == lt_avg_pooling)
    {
      uint16_t input_columns = LAYER_OUTPUT_WIDTH[current_layer_index - 1];
      uint16_t input_rows = LAYER_OUTPUT_HEIGHT[current_layer_index - 1];
      uint16_t input_depth = LAYER_OUTPUT_DEPTH[current_layer_index - 1];

      uint16_t output_columns = LAYER_OUTPUT_WIDTH[current_layer_index];
      uint16_t output_rows = LAYER_OUTPUT_HEIGHT[current_layer_index];
      uint16_t output_depth = LAYER_OUTPUT_DEPTH[current_layer_index];

      uint16_t pool_size_width = POOL_WIDTH[current_layer_index];
      uint16_t pool_size_height = POOL_HEIGHT[current_layer_index];
      uint16_t horizontal_stride = HORIZONTAL_STRIDE[current_layer_index];
      uint16_t vertical_stride = VERTICAL_STRIDE[current_layer_index];
      uint8_t padding = PADDING[current_layer_index];
      uint8_t pooling_type = LAYER_TYPE[current_layer_index - 1];
      input = pooling_apply(input, input_columns, input_rows, input_depth, pooling_type, pool_size_width, pool_size_height, horizontal_stride, vertical_stride, padding, output_columns, output_rows);
    }
    //Activation
    else if (LAYER_TYPE[current_layer_index - 1] == lt_activation)
    {
      uint16_t input_columns = LAYER_OUTPUT_WIDTH[current_layer_index - 1];
      uint16_t input_rows = LAYER_OUTPUT_HEIGHT[current_layer_index - 1];
      uint16_t input_depth = LAYER_OUTPUT_DEPTH[current_layer_index -1];
      uint8_t activation = ACTIVATION_FUNCTION[current_layer_index - 1];
      input = activation_apply(input, input_columns, input_rows, input_depth, activation);
    }

    /* We do not need to do anything for flatten layers since we're already using a flattened array structure
      and the calculation of the proper sizing is already performed by the python backend. */
  }

  return input;
}