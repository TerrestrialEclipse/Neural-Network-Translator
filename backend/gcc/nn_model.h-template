
/* Defines the number of layers. */
const uint16_t NUMBER_OF_LAYERS = ###numberLayers###;

/* */
//? NEW
const uint16_t LAYER_OUTPUT_WIDTH[###numberLayers###] = ###layerOutputWidth###;

/**/
//? NEW
const uint16_t LAYER_OUTPUT_HEIGHT[###numberLayers###] = ###layerOutputHeight###;

const uint16_t LAYER_OUTPUT_DEPTH[###numberLayers###] = ###layerOutputDepth###;

/* Defines the layer types */
const uint8_t LAYER_TYPE[###dimNumberLayers###] = ###layerTypes###;

/*
   Defines the activation functions for each layer as follows:
   0: Linera
   1: Sigmoid
   2: Relu
   3: TanH
   4: Softmax
*/
const uint8_t ACTIVATION_FUNCTION[###dimNumberLayers###] = ###activationFunctions###;

/*  Defines the index in which the first weight-element of the layer is present. */
const uint16_t WEIGHTS_START_INDEX[###dimNumberLayers###] = ###indicesWeights###;

/* Defines the index in which the first weight-element of the layer is present. */
const uint16_t BIASES_START_INDEX[###dimNumberLayers###] = ###indicesBias###;

/* Defines whether bias values should be applied to the layer. */
const uint8_t BIAS_ENABLED[###dimNumberLayers###] = ###useBias###;

/* Holds the weights for each layer as flatted one-dimensional array. */
const float WEIGHTS[###dimWeights###] = ###weights###;

/* Holds the biases for each layer as flatted one-dimensional array. */
const float BIASES[###dimBias###] = ###bias###;

//? NEW
const uint16_t POOL_WIDTH[###dimNumberLayers###] = ###poolWidth###;

//? NEW
const uint16_t POOL_HEIGHT[###dimNumberLayers###] = ###poolHeight###;

//? NEW
const uint16_t HORIZONTAL_STRIDE[###dimNumberLayers###] = ###horizontalStride###;

//? NEW
const uint16_t VERTICAL_STRIDE[###dimNumberLayers###] = ###verticalStride###;

//? NEW
const uint8_t PADDING[###dimNumberLayers###] = ###padding###;


float * predict(float input[]);

static float * dense_apply(float * input, uint16_t number_of_previous_units, uint16_t number_of_current_units, const float weights[], uint16_t weights_start_index, const float biases[], uint16_t bias_start_index, uint8_t bias_enabled, uint8_t activation_function);
static float * padding_zero_apply(float * input, uint16_t input_width, uint16_t input_height, uint16_t pool_width, uint16_t pool_height);
static float * padding_values_apply(float * input, uint16_t input_width, uint16_t input_height, uint16_t number_of_padding_layers);
static float * pooling_avg_apply(float * input, uint16_t input_width, uint16_t pool_width, uint16_t pool_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint16_t output_width, uint16_t output_height);
static float * pooling_max_apply(float * input, uint16_t input_width, uint16_t pool_width, uint16_t pool_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint16_t output_width, uint16_t output_height);
static float * pooling_apply(uint8_t pooling_type,float *input, uint16_t input_width, uint16_t input_height,uint16_t output_width, uint16_t output_height, uint16_t pool_width, uint16_t pool_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint8_t padding);
static float * activation_apply(float * values, uint16_t values_length, uint8_t act_func, float denominator);
static float * convolution_apply(float * input, uint16_t input_width, uint16_t input_height, const float kernel[], uint16_t kernel_width, uint16_t kernel_height, uint16_t horizontal_stride, uint16_t vertical_stride, uint16_t output_width, uint16_t output_height);

static uint16_t pooling_calculate_output_size(uint16_t input_dimension, uint16_t pool_dimension, uint16_t padding_dimension, uint16_t stride_size);
static uint16_t padding_calculate_size(uint16_t pool_dimension);
static uint16_t padding_calculate_output_size(uint16_t input_dimension, uint16_t padding_dimension);

static float activation_function_apply(uint8_t act_func, float value, float denominator);
