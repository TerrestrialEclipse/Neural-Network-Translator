{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TH NÃ¼rnberg - Neural Network Translator - Christoph Brandl, Philipp Grandeit\n",
    "\n",
    "# Evaluate the Performance of the sample Keras Neural Networks for the Neural-Network-Translator\n",
    "\n",
    "This jupyter notebooks provides everything you need to evaluate the performance of the sample neural networks with TensorFlow (Keras) which you can later translate with the neural network translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an out directory to save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('out'):\n",
    "    os.makedirs('out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Pooling 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will build a neural network only consisting of one single average pooling 1D layer. The created model can then be used to validate the implementation of the average pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, we will prepare all the necessary data to build and \"train\" this simple neural network. Since the pooling layer is a linear process no learning will take place in this step. Hence, it does not matter which data we will use. The only thing we have to make sure is that the shape of the input and output data is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([[1,2,3,4,5,6]])\n",
    "data_y = np.array([[1,2,3]])\n",
    "\n",
    "data_x = scale(data_x,axis=0)\n",
    "data_x = tf.reshape(data_x,[-1,6,1])\n",
    "data_y = tf.reshape(data_y,[-1,3,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will actually build the model. You can change the different parameters of the layer to play around with different settings and fully test the average pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.AveragePooling1D(pool_size=2, strides=2 ,padding='valid', input_shape = (6,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet will throw a warning since we do not actually learn anything. Since this is the planned behaviour, the warning can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_x, data_y, epochs=1, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save our model to a .h5 file to translate it with the neural network translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('out/avg_pool_1d.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will prepare a sample input to check if the result of our build model is equal to the result of our translated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.array([[9,119,80,35,0,29],\n",
    "                        [94,33,146,36.6,0.254,51],\n",
    "                        [10,125,70,26,115,31.1],\n",
    "                        [76,0,0,39.4,0.257,43],\n",
    "                        [1,97,66,15,140,23.2],\n",
    "                        [82,19,110,22.2,0.245,57],\n",
    "                        [5,117,92,0,0,34.1],\n",
    "                        [75,26,0,36,0.546,60],\n",
    "                        [3,158,76,36,245,31.6],\n",
    "                        [58,11,54,24.8,0.267,22]\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more convenient, we will prepare the input data so that we can simply copy and paste the values in the serial dialog of our arduino ide. We can now simply copy and paste the values inbetween of the square brackets [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(input_array[i].flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will perform the prediction with the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for i in range(0,10):\n",
    "    input = tf.reshape(input_array[i],[1,6,1])\n",
    "    time_before = int(round(time.time_ns() / 1000))\n",
    "    predictions = model.predict(input)\n",
    "    time_after = int(round(time.time_ns() / 1000))\n",
    "    print(predictions)\n",
    "    total_duration += time_after - time_before\n",
    "    print(\"process time in microseconds: \" + str(time_after - time_before))\n",
    "average_duration = float(total_duration)/10\n",
    "print(\"total_duration: \" + str(total_duration))\n",
    "print(\"average_duration: \" + str(average_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the output of our trained model with the output of the neural network translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Pooling 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we repeat the building process but instead of a 1D average pooling layer we will build a neural network only consisting of one single average pooling 2D layer. The created model can then be again used to validate the implementation of the average pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step we will prepare all the necessary data to build and train this simple neural network. Since the pooling layer is a linear process no learning will take place in this step. Hence, it does not matter which data we will use. The only thing we have to make sure is that the shape of the input and output data is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([[1,1,1], [1,1,1], [1,1,1], [1,1,1], [1,1,1]])\n",
    "data_y = np.array([[[1],[1]],[[1],[1]],[[1],[1]],[[1],[1]]])\n",
    "\n",
    "data_x = scale(data_x,axis=0)\n",
    "data_x = tf.reshape(data_x,[-1,5,3,1])\n",
    "data_y = tf.reshape(data_y,[-1,4,2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will actually build the model. You can change the different parameters of the layer to play around with different settings and fully test the average pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.AveragePooling2D(pool_size=(2,2), strides=(1,1),padding='valid', input_shape = (5,3,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet will throw a warning since we do not actually learn anything. Since this is the planned behaviour, the warning can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_x, data_y, epochs=1, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save our model to a .h5 file to translate it with the neural network translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('out/avg_pool_2d.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will prepare a sample input to check if the result of our build model is equal to the result of our translated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.array([[[26,1,37], [115,189,31.3], [31.3,29.6,103], [0.205,0,83], [41,36,2.2]],\n",
    "                        [[9,119,80], [35,0,29], [94,33,146], [36.6,0.254,51], [10,125,70]],\n",
    "                        [[26,115,31.1], [76,0,0], [39.4,0.257,43], [1,97,66], [15,140,23.2]],\n",
    "                        [[82,19,110], [22.2,0.245,57], [5,117,92], [0,0,34.1], [75,26,0]],\n",
    "                        [[36,0.546,60], [3,158,76], [36,245,31.6], [58,11,54], [24.8,0.267,22]],\n",
    "                        [[1,79,60], [42,48,43.5], [0.678,23,2], [75,64,24], [55,29.7,0.37]],\n",
    "                        [[33,8,179], [72,42,130], [32.7,0.719,36], [6,85,78], [0,0,31.2]],\n",
    "                        [[0.382,42,0], [129,110,46], [130,67.1,0.319], [26,5,143], [78,0,0]],\n",
    "                        [[45,0.19,47], [5,130,82], [0,0,39.1], [0.956,37,6], [87,80,0]],\n",
    "                        [[0,23.2,0.084], [0,119,64], [18,92,34.9], [0.725,23,1], [0,74,20]]\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more convenient, we will prepare the input data so that we can simply copy and paste the values in the serial dialog of our arduino ide. We can now simply copy and paste the values inbetween of the square brackets [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(input_array[i].flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will perform the prediction with the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for i in range(0,10):\n",
    "    input = tf.reshape(input_array[i],[1,5,3,1])\n",
    "    time_before = int(round(time.time_ns() / 1000))\n",
    "    predictions = model.predict(input)\n",
    "    time_after = int(round(time.time_ns() / 1000))\n",
    "    print(predictions)\n",
    "    total_duration += time_after - time_before\n",
    "    print(\"process time in microseconds: \" + str(time_after - time_before))    \n",
    "average_duration = float(total_duration)/10\n",
    "print(\"total_duration: \" + str(total_duration))\n",
    "print(\"average_duration: \" + str(average_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the output of our trained model with the output of the neural network translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will build a neural network only consisting of one single max pooling 1D layer. The created model can then be used to validate the implementation of the average pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step we will prepare all the necessary data to build and train this simple neural network. Since the pooling layer is a linear process no learning will take place in this step. Hence, it does not matter which data we will use. The only thing we have to make sure is that the shape of the input and output data is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([[1,2,3,4,5,6]])\n",
    "data_y = np.array([[1,2,3]])\n",
    "\n",
    "data_x = scale(data_x,axis=0)\n",
    "data_x = tf.reshape(data_x,[-1,6,1])\n",
    "data_y = tf.reshape(data_y,[-1,3,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will actually build the model. You can change the different parameters of the layer to play around with different settings and fully test the average pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2, strides=2 ,padding='valid', input_shape = (6,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet will throw a warning since we do not actually learn anything. Since this is the planned behaviour, the warning can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_x, data_y, epochs=1, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save our model to a .h5 file to translate it with the neural network translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('out/max_pool_1d.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will prepare a sample input to check if the result of our build model is equal to the result of our translated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.array([[9,119,80,35,0,29],\n",
    "                        [94,33,146,36.6,0.254,51],\n",
    "                        [10,125,70,26,115,31.1],\n",
    "                        [76,0,0,39.4,0.257,43],\n",
    "                        [1,97,66,15,140,23.2],\n",
    "                        [82,19,110,22.2,0.245,57],\n",
    "                        [5,117,92,0,0,34.1],\n",
    "                        [75,26,0,36,0.546,60],\n",
    "                        [3,158,76,36,245,31.6],\n",
    "                        [58,11,54,24.8,0.267,22]\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more convenient, we will prepare the input data so that we can simply copy and paste the values in the serial dialog of our arduino ide. We can now simply copy and paste the values inbetween of the square brackets [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(input_array[i].flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will perform the prediction with the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for i in range(0,10):\n",
    "    input = tf.reshape(input_array[i],[1,6,1])\n",
    "    time_before = int(round(time.time_ns() / 1000))\n",
    "    predictions = model.predict(input)\n",
    "    time_after = int(round(time.time_ns() / 1000))\n",
    "    print(predictions)\n",
    "    total_duration += time_after - time_before\n",
    "    print(\"process time in microseconds: \" + str(time_after - time_before))\n",
    "average_duration = float(total_duration)/10\n",
    "print(\"total_duration: \" + str(total_duration))\n",
    "print(\"average_duration: \" + str(average_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the output of our trained model with the output of the neural network translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we repeat the building process but instead of a 1D max pooling layer we will build a neural network only consisting of one single max pooling 2D layer. The created model can then be again used to validate the implementation of the average pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step we will prepare all the necessary data to build and train this simple neural network. Since the pooling layer is a linear process no learning will take place in this step. Hence, it does not matter which data we will use. The only thing we have to make sure is that the shape of the input and output data is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([[1,1,1], [1,1,1], [1,1,1], [1,1,1], [1,1,1]])\n",
    "data_y = np.array([[[1],[1]],[[1],[1]],[[1],[1]],[[1],[1]]])\n",
    "\n",
    "data_x = scale(data_x,axis=0)\n",
    "data_x = tf.reshape(data_x,[-1,5,3,1])\n",
    "data_y = tf.reshape(data_y,[-1,4,2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will actually build the model. You can change the different parameters of the layer to play around with different settings and fully test the average pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(1,1),padding='valid', input_shape = (5,3,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet will throw a warning since we do not actually learn anything. Since this is the planned behaviour, the warning can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_x, data_y, epochs=1, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save our model to a .h5 file to translate it with the neural network translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('out/max_pool_2d.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will prepare a sample input to check if the result of our build model is equal to the result of our translated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.array([[[26,1,37], [115,189,31.3], [31.3,29.6,103], [0.205,0,83], [41,36,2.2]],\n",
    "                        [[9,119,80], [35,0,29], [94,33,146], [36.6,0.254,51], [10,125,70]],\n",
    "                        [[26,115,31.1], [76,0,0], [39.4,0.257,43], [1,97,66], [15,140,23.2]],\n",
    "                        [[82,19,110], [22.2,0.245,57], [5,117,92], [0,0,34.1], [75,26,0]],\n",
    "                        [[36,0.546,60], [3,158,76], [36,245,31.6], [58,11,54], [24.8,0.267,22]],\n",
    "                        [[1,79,60], [42,48,43.5], [0.678,23,2], [75,64,24], [55,29.7,0.37]],\n",
    "                        [[33,8,179], [72,42,130], [32.7,0.719,36], [6,85,78], [0,0,31.2]],\n",
    "                        [[0.382,42,0], [129,110,46], [130,67.1,0.319], [26,5,143], [78,0,0]],\n",
    "                        [[45,0.19,47], [5,130,82], [0,0,39.1], [0.956,37,6], [87,80,0]],\n",
    "                        [[0,23.2,0.084], [0,119,64], [18,92,34.9], [0.725,23,1], [0,74,20]]\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more convenient, we will prepare the input data so that we can simply copy and paste the values in the serial dialog of our arduino ide. We can now simply copy and paste the values inbetween of the square brackets [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(input_array[i].flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will perform the prediction with the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for i in range(0,10):\n",
    "    input = tf.reshape(input_array[i],[1,5,3,1])\n",
    "    time_before = int(round(time.time_ns() / 1000))\n",
    "    predictions = model.predict(input)\n",
    "    time_after = int(round(time.time_ns() / 1000))\n",
    "    print(predictions)\n",
    "    total_duration += time_after - time_before\n",
    "    print(\"process time in microseconds: \" + str(time_after - time_before))    \n",
    "average_duration = float(total_duration)/10\n",
    "print(\"total_duration: \" + str(total_duration))\n",
    "print(\"average_duration: \" + str(average_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the output of our trained model with the output of the neural network translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Layer Diabates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will create a sample neural network which consists of two dense layers. Thereby, the provided diabetes.csv file ist used as training and test data. In the first step, we will load the dataset and split it into the feature vector and the result vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"diabetes.csv\", delimiter=\",\", skiprows=1 )\n",
    "diabetes_X = dataset[:,0:8]\n",
    "diabetes_Y = dataset[:,8]\n",
    "diabetes_X = scale(diabetes_X,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create a sequential neural network consisting of two dense layers with sigmoid activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(8, input_dim=8, activation='sigmoid'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(diabetes_X, diabetes_Y, epochs=300, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('out/2_layer_diabetes.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we extract the first line of the dataset and create a sample feature vector to perform a prediction. In addition to that, we will print the input values in order to be able to copy them to the model flashed on the arduino. Finally, the prediction results as well as the processing time will be printed to combare the framework with the translated model created by the neural network translator for the arduino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for i in range(0,10):\n",
    "    input_array = dataset[0][0:8]\n",
    "    input = tf.reshape(input_array,[1,8])\n",
    "    print(\"Input: \" + str(np.array(input[0]).flatten().tolist()))\n",
    "    time_before = int(round(time.time_ns() / 1000))\n",
    "    predictions = model.predict(input)\n",
    "    time_after = int(round(time.time_ns() / 1000))\n",
    "    print(\"Preditction: \" + str(predictions))\n",
    "    total_duration += time_after - time_before\n",
    "    print(\"Processing time in microseconds: \" + str(time_after - time_before))\n",
    "average_duration = float(total_duration)/10\n",
    "print(\"total_duration: \" + str(total_duration))\n",
    "print(\"average_duration: \" + str(average_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Layer Diabates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the 2-layer-diabetes neural network, a 3-layer-diabetes neural network can be created to test the neural network translator. Not only does this neural network have an additional dense layer, but it also uses different activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"diabetes.csv\", delimiter=\",\", skiprows=1 )\n",
    "dataset[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and save the neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(16, input_dim=8, activation='relu'))\n",
    "model.add(keras.layers.Dense(8, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(diabetes_X, diabetes_Y, epochs=300, batch_size=10)\n",
    "model.save('out/3_layer_diabetes.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for i in range(1,10):\n",
    "    input_array = dataset[0][0:8]\n",
    "    input = tf.reshape(input_array,[1,8])\n",
    "    print(\"Input: \" + str(np.array(input[0]).flatten().tolist()))\n",
    "    time_before = int(round(time.time_ns() / 1000))\n",
    "    predictions = model.predict(input)\n",
    "    time_after = int(round(time.time_ns() / 1000))\n",
    "    print(\"Preditction: \" + str(predictions))\n",
    "    total_duration += time_after - time_before\n",
    "    print(\"Processing time in microseconds: \" + str(time_after - time_before))\n",
    "average_duration = float(total_duration)/10\n",
    "print(\"total_duration: \" + str(total_duration))\n",
    "print(\"average_duration: \" + str(average_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
