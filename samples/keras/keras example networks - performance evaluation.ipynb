{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TH NÃ¼rnberg - Neural Network Translator - Christoph Brandl, Philipp Grandeit\n",
    "\n",
    "# Evaluate the Performance of the sample Keras Neural Networks for the Neural-Network-Translator\n",
    "\n",
    "This jupyter notebooks is an addition to the performance analysis in the project report. It provides all information about the models as well as the used input values and the function calls. Therefore, this notebook is suitable for repeating the performance tests. For a detailed description of the neural networks, please see the notebook \"keras example networks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an out directory to save the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('out'):\n",
    "    os.makedirs('out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Pooling 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will build a neural network only consisting of one single average pooling 1D layer. The created model can then be flashed on the Arduino microcontroller to evaluate the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([[1,2,3,4,5,6]])\n",
    "data_y = np.array([[1,2,3]])\n",
    "\n",
    "data_x = scale(data_x,axis=0)\n",
    "data_x = tf.reshape(data_x,[-1,6,1])\n",
    "data_y = tf.reshape(data_y,[-1,3,1])\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.AveragePooling1D(pool_size=2, strides=2 ,padding='valid', input_shape = (6,1)))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(data_x, data_y, epochs=1, batch_size=10)\n",
    "\n",
    "model.save('out/avg_pool_1d.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will prepare ten sample inputs to verify the results as well as the prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.array([[9,119,80,35,0,29],\n",
    "                        [94,33,146,36.6,0.254,51],\n",
    "                        [10,125,70,26,115,31.1],\n",
    "                        [76,0,0,39.4,0.257,43],\n",
    "                        [1,97,66,15,140,23.2],\n",
    "                        [82,19,110,22.2,0.245,57],\n",
    "                        [5,117,92,0,0,34.1],\n",
    "                        [75,26,0,36,0.546,60],\n",
    "                        [3,158,76,36,245,31.6],\n",
    "                        [58,11,54,24.8,0.267,22]\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more convenient, we will prepare the input data so that we can simply copy and paste the values in the serial dialog of our Arduino IDE. We can now simply copy and paste the values inbetween of the square brackets [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(input_array[i].flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will perform the predictions\n",
    "with the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for i in range(0,10):\n",
    "    input = tf.reshape(input_array[i],[1,6,1])\n",
    "    time_before = int(round(time.time_ns() / 1000))\n",
    "    predictions = model.predict(input)\n",
    "    time_after = int(round(time.time_ns() / 1000))\n",
    "    print(predictions)\n",
    "    total_duration += time_after - time_before\n",
    "    print(\"process time in microseconds: \" + str(time_after - time_before))\n",
    "average_duration = float(total_duration)/10\n",
    "print(\"total_duration: \" + str(total_duration))\n",
    "print(\"average_duration: \" + str(average_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the output and the duration of the prediction of our trained model with the output of the neural network translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Pooling 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we repeat the building process but instead of a 1D average pooling layer, we will build a neural network only consisting of one single average pooling 2D layer. The created model can then be again used to validate the performance of the average pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([[1,1,1], [1,1,1], [1,1,1], [1,1,1], [1,1,1]])\n",
    "data_y = np.array([[[1],[1]],[[1],[1]],[[1],[1]],[[1],[1]]])\n",
    "\n",
    "data_x = scale(data_x,axis=0)\n",
    "data_x = tf.reshape(data_x,[-1,5,3,1])\n",
    "data_y = tf.reshape(data_y,[-1,4,2,1])\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.AveragePooling2D(pool_size=(2,2), strides=(1,1),padding='valid', input_shape = (5,3,1)))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(data_x, data_y, epochs=1, batch_size=10)\n",
    "\n",
    "model.save('out/avg_pool_2d.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will again prepare ten input samples to check if the result of our build model is equal to the result of our translated model and how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.array([[[26,1,37], [115,189,31.3], [31.3,29.6,103], [0.205,0,83], [41,36,2.2]],\n",
    "                        [[9,119,80], [35,0,29], [94,33,146], [36.6,0.254,51], [10,125,70]],\n",
    "                        [[26,115,31.1], [76,0,0], [39.4,0.257,43], [1,97,66], [15,140,23.2]],\n",
    "                        [[82,19,110], [22.2,0.245,57], [5,117,92], [0,0,34.1], [75,26,0]],\n",
    "                        [[36,0.546,60], [3,158,76], [36,245,31.6], [58,11,54], [24.8,0.267,22]],\n",
    "                        [[1,79,60], [42,48,43.5], [0.678,23,2], [75,64,24], [55,29.7,0.37]],\n",
    "                        [[33,8,179], [72,42,130], [32.7,0.719,36], [6,85,78], [0,0,31.2]],\n",
    "                        [[0.382,42,0], [129,110,46], [130,67.1,0.319], [26,5,143], [78,0,0]],\n",
    "                        [[45,0.19,47], [5,130,82], [0,0,39.1], [0.956,37,6], [87,80,0]],\n",
    "                        [[0,23.2,0.084], [0,119,64], [18,92,34.9], [0.725,23,1], [0,74,20]]\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more convenient, we will prepare the input data so that we can simply copy and paste the values in the serial dialog of our Arduino IDE. We can now simply copy and paste the values inbetween of the square brackets [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(input_array[i].flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will perform the predictions with the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for i in range(0,10):\n",
    "    input = tf.reshape(input_array[i],[1,5,3,1])\n",
    "    time_before = int(round(time.time_ns() / 1000))\n",
    "    predictions = model.predict(input)\n",
    "    time_after = int(round(time.time_ns() / 1000))\n",
    "    print(predictions)\n",
    "    total_duration += time_after - time_before\n",
    "    print(\"process time in microseconds: \" + str(time_after - time_before))    \n",
    "average_duration = float(total_duration)/10\n",
    "print(\"total_duration: \" + str(total_duration))\n",
    "print(\"average_duration: \" + str(average_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the output and the runtime of our trained model with the output and the runtime of the neural network translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will build a neural network only consisting of one single max pooling 1D layer. The created model can then be used to validate the performance and the runtime of the max pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([[1,2,3,4,5,6]])\n",
    "data_y = np.array([[1,2,3]])\n",
    "\n",
    "data_x = scale(data_x,axis=0)\n",
    "data_x = tf.reshape(data_x,[-1,6,1])\n",
    "data_y = tf.reshape(data_y,[-1,3,1])\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2, strides=2 ,padding='valid', input_shape = (6,1)))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(data_x, data_y, epochs=1, batch_size=10)\n",
    "\n",
    "model.save('out/max_pool_1d.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will prepare ten sample inputs to check if the result of our build model is equal to the result of our translated model and how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.array([[9,119,80,35,0,29],\n",
    "                        [94,33,146,36.6,0.254,51],\n",
    "                        [10,125,70,26,115,31.1],\n",
    "                        [76,0,0,39.4,0.257,43],\n",
    "                        [1,97,66,15,140,23.2],\n",
    "                        [82,19,110,22.2,0.245,57],\n",
    "                        [5,117,92,0,0,34.1],\n",
    "                        [75,26,0,36,0.546,60],\n",
    "                        [3,158,76,36,245,31.6],\n",
    "                        [58,11,54,24.8,0.267,22]\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more convenient, we will prepare the input data so that we can simply copy and paste the values in the serial dialog of our Arduino IDE. We can now simply copy and paste the values inbetween of the square brackets [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(input_array[i].flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we will perform the predictions with the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for i in range(0,10):\n",
    "    input = tf.reshape(input_array[i],[1,6,1])\n",
    "    time_before = int(round(time.time_ns() / 1000))\n",
    "    predictions = model.predict(input)\n",
    "    time_after = int(round(time.time_ns() / 1000))\n",
    "    print(predictions)\n",
    "    total_duration += time_after - time_before\n",
    "    print(\"process time in microseconds: \" + str(time_after - time_before))\n",
    "average_duration = float(total_duration)/10\n",
    "print(\"total_duration: \" + str(total_duration))\n",
    "print(\"average_duration: \" + str(average_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the output and the runtime of our trained model with the output and the runtime of the neural network translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we repeat the building process but instead of a 1D max pooling layer we will build a neural network only consisting of one single max pooling 2D layer. The created model can then be again used to validate the implementation and the performance of the max pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([[1,1,1], [1,1,1], [1,1,1], [1,1,1], [1,1,1]])\n",
    "data_y = np.array([[[1],[1]],[[1],[1]],[[1],[1]],[[1],[1]]])\n",
    "\n",
    "data_x = scale(data_x,axis=0)\n",
    "data_x = tf.reshape(data_x,[-1,5,3,1])\n",
    "data_y = tf.reshape(data_y,[-1,4,2,1])\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(1,1),padding='valid', input_shape = (5,3,1)))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(data_x, data_y, epochs=1, batch_size=10)\n",
    "\n",
    "model.save('out/max_pool_2d.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step; we will prepare a sample of ten inputs to check if the result of our build model is equal to the result of our translated model and how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.array([[[26,1,37], [115,189,31.3], [31.3,29.6,103], [0.205,0,83], [41,36,2.2]],\n",
    "                        [[9,119,80], [35,0,29], [94,33,146], [36.6,0.254,51], [10,125,70]],\n",
    "                        [[26,115,31.1], [76,0,0], [39.4,0.257,43], [1,97,66], [15,140,23.2]],\n",
    "                        [[82,19,110], [22.2,0.245,57], [5,117,92], [0,0,34.1], [75,26,0]],\n",
    "                        [[36,0.546,60], [3,158,76], [36,245,31.6], [58,11,54], [24.8,0.267,22]],\n",
    "                        [[1,79,60], [42,48,43.5], [0.678,23,2], [75,64,24], [55,29.7,0.37]],\n",
    "                        [[33,8,179], [72,42,130], [32.7,0.719,36], [6,85,78], [0,0,31.2]],\n",
    "                        [[0.382,42,0], [129,110,46], [130,67.1,0.319], [26,5,143], [78,0,0]],\n",
    "                        [[45,0.19,47], [5,130,82], [0,0,39.1], [0.956,37,6], [87,80,0]],\n",
    "                        [[0,23.2,0.084], [0,119,64], [18,92,34.9], [0.725,23,1], [0,74,20]]\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more convenient, we will prepare the input data so that we can simply copy and paste the values in the serial dialog of our Arduino IDE. We can now simply copy and paste the values inbetween of the square brackets [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(input_array[i].flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will perform the predictions with the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for i in range(0,10):\n",
    "    input = tf.reshape(input_array[i],[1,5,3,1])\n",
    "    time_before = int(round(time.time_ns() / 1000))\n",
    "    predictions = model.predict(input)\n",
    "    time_after = int(round(time.time_ns() / 1000))\n",
    "    print(predictions)\n",
    "    total_duration += time_after - time_before\n",
    "    print(\"process time in microseconds: \" + str(time_after - time_before))    \n",
    "average_duration = float(total_duration)/10\n",
    "print(\"total_duration: \" + str(total_duration))\n",
    "print(\"average_duration: \" + str(average_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the output and the performance of our trained model with the output and the performance of the neural network translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Layer Diabates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will create a sample neural network which consists of two dense layers. Thereby, the provided diabetes.csv file is used as training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"diabetes.csv\", delimiter=\",\", skiprows=1 )\n",
    "diabetes_X = dataset[:,0:8]\n",
    "diabetes_Y = dataset[:,8]\n",
    "diabetes_X = scale(diabetes_X,axis=0)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(8, input_dim=8, activation='sigmoid'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(diabetes_X, diabetes_Y, epochs=300, batch_size=10)\n",
    "\n",
    "model.save('out/2_layer_diabetes.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the predictions are performed as well as the processing time will be measured to compare the framework with the translated model created by the neural network translator for the arduino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for i in range(1,11):\n",
    "    input_array = dataset[i][0:8]\n",
    "    input = tf.reshape(input_array,[1,8])\n",
    "    print(\"Input: \" + str(np.array(input[0]).flatten().tolist()))\n",
    "    time_before = int(round(time.time_ns() / 1000))\n",
    "    predictions = model.predict(input)\n",
    "    time_after = int(round(time.time_ns() / 1000))\n",
    "    print(\"Preditction: \" + str(predictions))\n",
    "    total_duration += time_after - time_before\n",
    "    print(\"Processing time in microseconds: \" + str(time_after - time_before))\n",
    "average_duration = float(total_duration)/10\n",
    "print(\"total_duration: \" + str(total_duration))\n",
    "print(\"average_duration: \" + str(average_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Layer Diabates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the 2-layer-diabetes neural network, a 3-layer-diabetes neural network can be created to test the neural network translator. Not only does this neural network have an additional dense layer, but it also uses different activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and save the neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"diabetes.csv\", delimiter=\",\", skiprows=1 )\n",
    "dataset[1:11]\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(16, input_dim=8, activation='relu'))\n",
    "model.add(keras.layers.Dense(8, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(diabetes_X, diabetes_Y, epochs=300, batch_size=10)\n",
    "model.save('out/3_layer_diabetes.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform predictions and measure runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for i in range(1,11):\n",
    "    input_array = dataset[i][0:8]\n",
    "    input = tf.reshape(input_array,[1,8])\n",
    "    print(\"Input: \" + str(np.array(input[0]).flatten().tolist()))\n",
    "    time_before = int(round(time.time_ns() / 1000))\n",
    "    predictions = model.predict(input)\n",
    "    time_after = int(round(time.time_ns() / 1000))\n",
    "    print(\"Preditction: \" + str(predictions))\n",
    "    total_duration += time_after - time_before\n",
    "    print(\"Processing time in microseconds: \" + str(time_after - time_before))\n",
    "average_duration = float(total_duration)/10\n",
    "print(\"total_duration: \" + str(total_duration))\n",
    "print(\"average_duration: \" + str(average_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
